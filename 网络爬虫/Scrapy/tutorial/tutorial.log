2017-08-18 14:43:35 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 14:43:35 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'LOG_FILE': 'tutorial.log'}
2017-08-18 14:43:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 14:43:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 14:43:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 14:43:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 14:43:35 [scrapy.core.engine] INFO: Spider opened
2017-08-18 14:43:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 14:43:35 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 14:43:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:43:35 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:43:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:43:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:43:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Highest%20Voted%20Questions%20-%20Stack%20Overflow.html> (referer: None)
2017-08-18 14:43:36 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 14:43:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 124660,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 6, 43, 36, 58041),
 'log_count/DEBUG': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 693784576,
 'memusage/startup': 693784576,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 18, 6, 43, 35, 920112)}
2017-08-18 14:43:36 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 14:45:41 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 14:45:41 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders', 'BOT_NAME': 'tutorial'}
2017-08-18 14:45:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 14:45:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 14:45:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 14:45:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 14:45:41 [scrapy.core.engine] INFO: Spider opened
2017-08-18 14:45:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 14:45:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 14:45:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.runoob.com/robots.txt> (referer: None)
2017-08-18 14:45:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.runoob.com/mongodb/mongodb-tutorial.html> (referer: None)
2017-08-18 14:45:41 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 14:45:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 463,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 16333,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 6, 45, 41, 764826),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 705822720,
 'memusage/startup': 705822720,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 18, 6, 45, 41, 387934)}
2017-08-18 14:45:41 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 14:55:18 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 14:55:18 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders']}
2017-08-18 14:55:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 14:55:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 14:55:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 14:55:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 14:55:19 [scrapy.core.engine] INFO: Spider opened
2017-08-18 14:55:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 14:55:19 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 14:55:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:55:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:55:19 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:55:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:55:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (failed 1 times): [Errno 2] No such file or directory: '/home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html'
2017-08-18 14:55:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (failed 2 times): [Errno 2] No such file or directory: '/home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html'
2017-08-18 14:55:19 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (failed 3 times): [Errno 2] No such file or directory: '/home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html'
2017-08-18 14:55:19 [scrapy.core.scraper] ERROR: Error downloading <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html'
2017-08-18 14:55:19 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 14:55:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/builtins.FileNotFoundError': 6,
 'downloader/request_bytes': 1542,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 6, 55, 19, 255310),
 'log_count/DEBUG': 7,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 772468736,
 'memusage/startup': 772468736,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/builtins.FileNotFoundError': 4,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 8, 18, 6, 55, 19, 34489)}
2017-08-18 14:55:19 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 14:55:40 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 14:55:40 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial', 'SPIDER_MODULES': ['tutorial.spiders'], 'LOG_FILE': 'tutorial.log'}
2017-08-18 14:55:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 14:55:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 14:55:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 14:55:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 14:55:41 [scrapy.core.engine] INFO: Spider opened
2017-08-18 14:55:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 14:55:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 14:55:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:55:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:55:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:55:41 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:55:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (failed 1 times): [Errno 2] No such file or directory: '/home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html'
2017-08-18 14:55:41 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (failed 2 times): [Errno 2] No such file or directory: '/home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html'
2017-08-18 14:55:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (failed 3 times): [Errno 2] No such file or directory: '/home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html'
2017-08-18 14:55:41 [scrapy.core.scraper] ERROR: Error downloading <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/home/lwq/Desktop/Python/tutorial/html/Books.htmlfile:///home/lwq/Desktop/Python/tutorial/html/Resources.html'
2017-08-18 14:55:41 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 14:55:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/builtins.FileNotFoundError': 6,
 'downloader/request_bytes': 1542,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 6, 55, 41, 284304),
 'log_count/DEBUG': 7,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 772972544,
 'memusage/startup': 772972544,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/builtins.FileNotFoundError': 4,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 8, 18, 6, 55, 41, 62322)}
2017-08-18 14:55:41 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 14:57:24 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 14:57:24 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True}
2017-08-18 14:57:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 14:57:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 14:57:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 14:57:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 14:57:24 [scrapy.core.engine] INFO: Spider opened
2017-08-18 14:57:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 14:57:24 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 14:57:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:57:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:57:24 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:57:24 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:57:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 14:57:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 14:57:24 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 14:57:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 6, 57, 24, 426275),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 779128832,
 'memusage/startup': 779128832,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 6, 57, 24, 292497)}
2017-08-18 14:57:24 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 14:57:39 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 14:57:39 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log', 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 14:57:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 14:57:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 14:57:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 14:57:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 14:57:39 [scrapy.core.engine] INFO: Spider opened
2017-08-18 14:57:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 14:57:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 14:57:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:57:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:57:39 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:57:39 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:57:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 14:57:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 14:57:39 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 14:57:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 6, 57, 39, 834495),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 779202560,
 'memusage/startup': 779202560,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 6, 57, 39, 703720)}
2017-08-18 14:57:39 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 14:58:13 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 14:58:13 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial', 'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log'}
2017-08-18 14:58:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 14:58:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 14:58:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 14:58:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 14:58:13 [scrapy.core.engine] INFO: Spider opened
2017-08-18 14:58:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 14:58:13 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 14:58:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:58:13 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:58:13 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:58:13 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 14:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 14:58:14 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 14:58:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 6, 58, 14, 51627),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 791310336,
 'memusage/startup': 791310336,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 6, 58, 13, 914953)}
2017-08-18 14:58:14 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 14:59:48 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 14:59:48 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 14:59:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 14:59:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 14:59:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 14:59:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 14:59:48 [scrapy.core.engine] INFO: Spider opened
2017-08-18 14:59:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 14:59:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 14:59:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:59:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:59:48 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:59:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 14:59:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 14:59:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 14:59:48 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 14:59:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 6, 59, 48, 433004),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 797700096,
 'memusage/startup': 797700096,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 6, 59, 48, 305058)}
2017-08-18 14:59:48 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:00:05 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:00:05 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log', 'BOT_NAME': 'tutorial', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True}
2017-08-18 15:00:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage']
2017-08-18 15:00:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:00:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:00:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:00:05 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:00:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:00:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:00:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:00:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:00:05 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:00:05 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:00:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:00:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:00:05 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:00:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 0, 5, 323910),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 797949952,
 'memusage/startup': 797949952,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 0, 5, 191714)}
2017-08-18 15:00:05 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:01:19 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:01:19 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log'}
2017-08-18 15:01:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 15:01:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:01:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:01:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:01:19 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:01:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:01:19 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:01:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:01:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:01:19 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:01:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:01:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:01:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:01:19 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:01:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 1, 19, 534574),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 803610624,
 'memusage/startup': 803610624,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 1, 19, 397248)}
2017-08-18 15:01:19 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:08:27 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:08:27 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'LOG_FILE': 'tutorial.log', 'BOT_NAME': 'tutorial', 'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 15:08:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 15:08:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:08:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:08:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:08:27 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:08:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:08:27 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:08:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:08:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:08:27 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:08:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:08:28 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 22, in parse
    print(li.a.text())
AttributeError: 'Selector' object has no attribute 'a'
2017-08-18 15:08:28 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 22, in parse
    print(li.a.text())
AttributeError: 'Selector' object has no attribute 'a'
2017-08-18 15:08:28 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:08:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 8, 28, 158219),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'memusage/max': 835657728,
 'memusage/startup': 835657728,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/AttributeError': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 8, 27, 923691)}
2017-08-18 15:08:28 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:08:54 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:08:54 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True}
2017-08-18 15:08:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 15:08:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:08:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:08:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:08:54 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:08:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:08:54 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:08:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:08:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:08:54 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:08:54 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:08:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:08:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:08:55 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:08:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 8, 55, 72789),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 835862528,
 'memusage/startup': 835862528,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 8, 54, 934259)}
2017-08-18 15:08:55 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:10:27 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:10:27 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'LOG_FILE': 'tutorial.log', 'BOT_NAME': 'tutorial'}
2017-08-18 15:10:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 15:10:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:10:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:10:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:10:27 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:10:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:10:27 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:10:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:10:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:10:27 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:10:27 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:10:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:10:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:10:27 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 22, in parse
    print(li.text())
AttributeError: text
2017-08-18 15:10:27 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 22, in parse
    print(li.text())
AttributeError: text
2017-08-18 15:10:27 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:10:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 10, 27, 873512),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'memusage/max': 839077888,
 'memusage/startup': 839077888,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/AttributeError': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 10, 27, 638025)}
2017-08-18 15:10:27 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:10:42 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:10:42 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True}
2017-08-18 15:10:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 15:10:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:10:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:10:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:10:42 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:10:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:10:42 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:10:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:10:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:10:42 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:10:42 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:10:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:10:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:10:42 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 22, in parse
    print(type(li.text()))
AttributeError: text
2017-08-18 15:10:42 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 22, in parse
    print(type(li.text()))
AttributeError: text
2017-08-18 15:10:42 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:10:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 10, 42, 527510),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'memusage/max': 839323648,
 'memusage/startup': 839323648,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/AttributeError': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 10, 42, 294142)}
2017-08-18 15:10:42 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:31:39 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:31:39 [scrapy.utils.log] INFO: Overridden settings: {'LOG_FILE': 'tutorial.log', 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'tutorial.spiders', 'BOT_NAME': 'tutorial', 'SPIDER_MODULES': ['tutorial.spiders']}
2017-08-18 15:31:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 15:31:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:31:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:31:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:31:39 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:31:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:31:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:31:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:31:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:31:39 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:31:39 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:31:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:31:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:31:39 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:31:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 31, 39, 696674),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 840904704,
 'memusage/startup': 840904704,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 31, 39, 559868)}
2017-08-18 15:31:39 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:42:24 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:42:24 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders']}
2017-08-18 15:42:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2017-08-18 15:42:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:42:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:42:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:42:25 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:42:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:42:25 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:42:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:42:25 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:42:25 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:42:25 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:42:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:42:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:42:25 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 23, in parse
    item['title'] = li.css('a>text()').extact()[0]
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 246, in css
    return self.xpath(self._css2xpath(query))
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 249, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "/usr/local/lib/python3.5/dist-packages/cssselect/xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 355, in parse
    return list(parse_selector_group(stream))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 370, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 396, in parse_selector
    next_selector, pseudo_element = parse_simple_selector(stream)
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 477, in parse_simple_selector
    "Expected selector, got %s" % (peek,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected selector, got <DELIM '(' at 6>
2017-08-18 15:42:25 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 23, in parse
    item['title'] = li.css('a>text()').extact()[0]
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 246, in css
    return self.xpath(self._css2xpath(query))
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 249, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "/usr/local/lib/python3.5/dist-packages/cssselect/xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 355, in parse
    return list(parse_selector_group(stream))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 370, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 396, in parse_selector
    next_selector, pseudo_element = parse_simple_selector(stream)
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 477, in parse_simple_selector
    "Expected selector, got %s" % (peek,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected selector, got <DELIM '(' at 6>
2017-08-18 15:42:25 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:42:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 42, 25, 288645),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'memusage/max': 848547840,
 'memusage/startup': 848547840,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/SelectorSyntaxError': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 42, 25, 55872)}
2017-08-18 15:42:25 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:43:34 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:43:34 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'LOG_FILE': 'tutorial.log', 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 15:43:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 15:43:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:43:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:43:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:43:34 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:43:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:43:34 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:43:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:43:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:43:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:43:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:43:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:43:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:43:34 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 23, in parse
    item['title'] = li.css('a>text()').extract()[0]
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 246, in css
    return self.xpath(self._css2xpath(query))
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 249, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "/usr/local/lib/python3.5/dist-packages/cssselect/xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 355, in parse
    return list(parse_selector_group(stream))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 370, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 396, in parse_selector
    next_selector, pseudo_element = parse_simple_selector(stream)
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 477, in parse_simple_selector
    "Expected selector, got %s" % (peek,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected selector, got <DELIM '(' at 6>
2017-08-18 15:43:34 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 23, in parse
    item['title'] = li.css('a>text()').extract()[0]
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 246, in css
    return self.xpath(self._css2xpath(query))
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 249, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "/usr/local/lib/python3.5/dist-packages/cssselect/xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 355, in parse
    return list(parse_selector_group(stream))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 370, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 396, in parse_selector
    next_selector, pseudo_element = parse_simple_selector(stream)
  File "/usr/local/lib/python3.5/dist-packages/cssselect/parser.py", line 477, in parse_simple_selector
    "Expected selector, got %s" % (peek,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected selector, got <DELIM '(' at 6>
2017-08-18 15:43:35 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:43:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 43, 35, 23701),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'memusage/max': 848785408,
 'memusage/startup': 848785408,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/SelectorSyntaxError': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 43, 34, 787749)}
2017-08-18 15:43:35 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:56:11 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:56:11 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log', 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 15:56:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 15:56:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:56:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:56:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:56:11 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:56:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:56:11 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:56:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:56:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:56:11 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:56:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:56:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:56:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:56:11 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:56:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 56, 11, 634456),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 855244800,
 'memusage/startup': 855244800,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 56, 11, 495206)}
2017-08-18 15:56:11 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:59:05 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:59:05 [scrapy.utils.log] INFO: Overridden settings: {'LOG_FILE': 'tutorial.log', 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial', 'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 15:59:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2017-08-18 15:59:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:59:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:59:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:59:05 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:59:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:59:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:59:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:05 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:05 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:59:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:59:05 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:59:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 59, 5, 691119),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 855244800,
 'memusage/startup': 855244800,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 59, 5, 548159)}
2017-08-18 15:59:05 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:59:38 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:59:38 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'LOG_FILE': 'tutorial.log'}
2017-08-18 15:59:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2017-08-18 15:59:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:59:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:59:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:59:38 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:59:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:59:38 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:59:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:38 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:38 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:59:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:59:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:59:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 59, 38, 685150),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 856104960,
 'memusage/startup': 856104960,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 59, 38, 537460)}
2017-08-18 15:59:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 15:59:52 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 15:59:52 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders']}
2017-08-18 15:59:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2017-08-18 15:59:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 15:59:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 15:59:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 15:59:52 [scrapy.core.engine] INFO: Spider opened
2017-08-18 15:59:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 15:59:52 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 15:59:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:52 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 15:59:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 15:59:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 15:59:52 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 15:59:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 7, 59, 52, 515764),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 856268800,
 'memusage/startup': 856268800,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 7, 59, 52, 373440)}
2017-08-18 15:59:52 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:02:04 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:02:04 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders']}
2017-08-18 16:02:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 16:02:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:02:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:02:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:02:04 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:02:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:02:04 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:02:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:04 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:04 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:02:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:02:04 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:02:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 2, 4, 584189),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 859975680,
 'memusage/startup': 859975680,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 2, 4, 446310)}
2017-08-18 16:02:04 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:02:37 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:02:37 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'LOG_FILE': 'tutorial.log', 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 16:02:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage']
2017-08-18 16:02:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:02:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:02:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:02:37 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:02:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:02:37 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:02:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:37 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:37 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:02:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:02:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 2, 38, 2386),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 859979776,
 'memusage/startup': 859979776,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 2, 37, 860669)}
2017-08-18 16:02:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:02:45 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:02:45 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log'}
2017-08-18 16:02:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 16:02:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:02:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:02:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:02:45 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:02:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:02:45 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:02:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:45 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:45 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:02:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:02:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:02:45 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:02:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 2, 45, 904907),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 860033024,
 'memusage/startup': 860033024,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 2, 45, 762581)}
2017-08-18 16:02:45 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:04:32 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:04:32 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'LOG_FILE': 'tutorial.log', 'ROBOTSTXT_OBEY': True}
2017-08-18 16:04:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 16:04:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:04:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:04:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:04:32 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:04:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:04:32 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:04:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:04:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:04:32 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:04:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:04:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:04:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:04:32 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:04:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 4, 32, 301214),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 866050048,
 'memusage/startup': 866050048,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 4, 32, 160869)}
2017-08-18 16:04:32 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:04:49 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:04:49 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'tutorial.spiders', 'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders']}
2017-08-18 16:04:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 16:04:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:04:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:04:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:04:49 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:04:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:04:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:04:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:04:49 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:04:49 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:04:49 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:04:50 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:04:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 4, 50, 35147),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 866050048,
 'memusage/startup': 866050048,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 4, 49, 898472)}
2017-08-18 16:04:50 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:05:43 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:05:43 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'LOG_FILE': 'tutorial.log'}
2017-08-18 16:05:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2017-08-18 16:05:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:05:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:05:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:05:43 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:05:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:05:43 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:05:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:05:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:05:43 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:05:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:05:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:05:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:05:43 [scrapy.core.scraper] ERROR: Spider error processing <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/Python/tutorial/tutorial/spiders/stackoverflow.py", line 25, in parse
    item['desc'] = li.xpath('/text()')[0]
  File "/usr/local/lib/python3.5/dist-packages/parsel/selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2017-08-18 16:05:43 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:05:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 5, 43, 565723),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 866050048,
 'memusage/startup': 866050048,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2017, 8, 18, 8, 5, 43, 335170)}
2017-08-18 16:05:43 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:05:48 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:05:48 [scrapy.utils.log] INFO: Overridden settings: {'LOG_FILE': 'tutorial.log', 'BOT_NAME': 'tutorial', 'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True}
2017-08-18 16:05:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 16:05:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:05:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:05:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:05:48 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:05:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:05:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:05:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:05:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:05:48 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:05:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:05:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:05:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:05:48 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:05:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 5, 48, 407388),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 866050048,
 'memusage/startup': 866050048,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 5, 48, 271376)}
2017-08-18 16:05:48 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:07:16 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:07:16 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 16:07:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 16:07:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:07:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:07:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:07:16 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:07:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:07:16 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:07:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:07:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:07:16 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:07:16 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:07:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:07:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:07:16 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:07:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 7, 16, 728147),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 866050048,
 'memusage/startup': 866050048,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 7, 16, 591669)}
2017-08-18 16:07:16 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:08:48 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:08:48 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'tutorial.spiders', 'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log'}
2017-08-18 16:08:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 16:08:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:08:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:08:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:08:48 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:08:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:08:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:08:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:08:48 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:08:48 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:08:48 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:08:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:08:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:08:49 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:08:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 8, 49, 83809),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 866050048,
 'memusage/startup': 866050048,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 8, 48, 946105)}
2017-08-18 16:08:49 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:10:43 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:10:43 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 16:10:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 16:10:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:10:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:10:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:10:43 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:10:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:10:43 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:10:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:10:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:10:43 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:10:43 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:10:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:10:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:10:43 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:10:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 10, 43, 607265),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 866050048,
 'memusage/startup': 866050048,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 10, 43, 471088)}
2017-08-18 16:10:43 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:11:34 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:11:34 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'LOG_FILE': 'tutorial.log', 'NEWSPIDER_MODULE': 'tutorial.spiders'}
2017-08-18 16:11:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 16:11:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:11:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:11:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:11:34 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:11:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:11:34 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:11:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:11:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:11:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:11:34 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:11:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:11:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:11:34 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:11:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 11, 34, 594948),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 866398208,
 'memusage/startup': 866398208,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 11, 34, 457772)}
2017-08-18 16:11:34 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:12:57 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:12:57 [scrapy.utils.log] INFO: Overridden settings: {'LOG_FILE': 'tutorial.log', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial'}
2017-08-18 16:12:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 16:12:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:12:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:12:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:12:57 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:12:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:12:57 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:12:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:12:57 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:12:57 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:12:57 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:12:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:12:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:12:58 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:12:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 12, 58, 84054),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 867221504,
 'memusage/startup': 867221504,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 12, 57, 945994)}
2017-08-18 16:12:58 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:13:10 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:13:10 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log'}
2017-08-18 16:13:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
2017-08-18 16:13:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:13:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:13:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:13:10 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:13:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:13:10 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:13:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:13:10 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:13:10 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:13:10 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:13:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:13:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:13:10 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:13:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 13, 10, 366487),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 867266560,
 'memusage/startup': 867266560,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 13, 10, 224147)}
2017-08-18 16:13:10 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:20:26 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:20:26 [scrapy.utils.log] INFO: Overridden settings: {'FEED_URI': 'temp.csv', 'FEED_FORMAT': 'csv', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'LOG_FILE': 'tutorial.log', 'BOT_NAME': 'tutorial'}
2017-08-18 16:20:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 16:20:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:20:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:20:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:20:26 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:20:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:20:26 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:20:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:20:26 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:20:26 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:20:26 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Wesley J. Chun; Prentice Hall PTR, 2001, ISBN 0130260363. For '
         'experienced developers to improve extant skills; professional level '
         'examples. Starts by introducing syntax, objects, error handling, '
         'functions, classes, built-ins. [Prentice Hall]',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130260363,00%2Ben-USS_01DBC.html',
 'title': ['Core Python Programming']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- The primary goal of this book is to promote object-oriented design '
         'using Python and to illustrate the use of the emerging '
         'object-oriented design patterns.\r\n'
         'A secondary goal of the book is to present mathematical tools just '
         'in time. Analysis techniques and proofs are presented as needed and '
         'in the proper context.',
 'link': 'http://www.brpreiss.com/books/opus7/html/book.html',
 'title': ['Data Structures and Algorithms with Object-Oriented Design '
           'Patterns in Python']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Mark Pilgrim, Guide to Python 3  and its differences from '
         'Python 2. Each chapter starts with a real code sample and explains '
         'it fully. Has a comprehensive appendix of all the syntactic and '
         'semantic changes in Python 3',
 'link': 'http://www.diveintopython.net/',
 'title': ['Dive Into Python 3']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- This book covers a wide range of topics. From raw TCP and UDP to '
         'encryption with TSL, and then to HTTP, SMTP, POP, IMAP, and ssh. It '
         'gives you a good understanding of each field and how to do '
         'everything on the network with Python.',
 'link': 'http://rhodesmill.org/brandon/2011/foundations-of-python-network-programming/',
 'title': ['Foundations of Python Network Programming']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Free Python books and tutorials.',
 'link': 'http://www.techbooksforfree.com/perlpython.shtml',
 'title': ['Free Python books']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Annotated list of free online books on Python scripting language. '
         'Topics range from beginner to advanced.',
 'link': 'http://www.freetechbooks.com/python-f6.html',
 'title': ['FreeTechBooks: Python Scripting Language']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Allen B. Downey, Jeffrey Elkner, Chris Meyers; Green Tea Press, '
         '2002, ISBN 0971677506. Teaches general principles of programming, '
         'via Python as subject language. Thorough, in-depth approach to many '
         'basic and intermediate programming topics. Full text online and '
         'downloads: HTML, PDF, PS, LaTeX. [Free, Green Tea Press]',
 'link': 'http://greenteapress.com/thinkpython/',
 'title': ['How to Think Like a Computer Scientist: Learning with Python']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Guido van Rossum, Fred L. Drake, Jr.; Network Theory Ltd., '
         '2003, ISBN 0954161769. Printed edition of official tutorial, for '
         'v2.x, from Python.org. [Network Theory, online]',
 'link': 'http://www.network-theory.co.uk/python/intro/',
 'title': ['An Introduction to Python']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Book by Alan Gauld with full text online. Introduction for those '
         'learning programming basics: terminology, concepts, methods to write '
         'code. Assumes no prior knowledge but basic computer skills.',
 'link': 'http://www.freenetpages.co.uk/hp/alan.gauld/',
 'title': ['Learn to Program Using Python']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Rashi Gupta; John Wiley and Sons, 2002, ISBN 0471219754. Covers '
         'language basics, use for CGI scripting, GUI development, network '
         'programming; shows why it is one of more sophisticated of popular '
         'scripting languages. [Wiley]',
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471219754.html',
 'title': ['Making Use of Python']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Magnus Lie Hetland; Apress LP, 2002, ISBN 1590590066. Readable '
         'guide to ideas most vital to new users, from basics common to high '
         'level languages, to more specific aspects, to a series of 10 ever '
         'more complex programs. [Apress]',
 'link': 'http://hetland.org/writing/practical-python/',
 'title': ['Practical Python']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Rytis Sileika, ISBN13: 978-1-4302-2605-5, Uses real-world '
         'system administration examples like manage devices with SNMP and '
         'SOAP, build a distributed monitoring system, manage web applications '
         'and parse complex log files, monitor and manage MySQL databases.',
 'link': 'http://sysadminpy.com/',
 'title': ['Pro Python System Administration']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- A Complete Introduction to the Python 3.',
 'link': 'http://www.qtrac.eu/py3book.html',
 'title': ['Programming in Python 3 (Second Edition)']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Dave Brueck, Stephen Tanner; John Wiley and Sons, 2001, ISBN '
         '0764548077. Full coverage, clear explanations, hands-on examples, '
         'full language reference; shows step by step how to use components, '
         'assemble them, form full-featured programs. [John Wiley and Sons]',
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0764548077.html',
 'title': ['Python 2.1 Bible']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- A step-by-step tutorial for OOP in Python 3, including discussion '
         'and examples of abstraction, encapsulation, information hiding, and '
         'raise, handle, define, and manipulate exceptions.',
 'link': 'https://www.packtpub.com/python-3-object-oriented-programming/book',
 'title': ['Python 3 Object Oriented Programming']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Guido van Rossum, Fred L. Drake, Jr.; Network Theory Ltd., '
         '2003, ISBN 0954161785. Printed edition of official language '
         'reference, for v2.x, from Python.org, describes syntax, built-in '
         'datatypes. [Network Theory, online]',
 'link': 'http://www.network-theory.co.uk/python/language/',
 'title': ['Python Language Reference Manual']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Thomas W. Christopher; Prentice Hall PTR, 2002, ISBN '
         '0130409561. Shows how to write large programs, introduces powerful '
         'design patterns that deliver high levels of robustness, scalability, '
         'reuse.',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130409561,00%2Ben-USS_01DBC.html',
 'title': ['Python Programming Patterns']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Richard Hightower; Addison-Wesley, 2002, 0201616165. Begins '
         'with Python basics, many exercises, interactive sessions. Shows '
         'programming novices concepts and practical methods. Shows '
         "programming experts Python's abilities and ways to interface with "
         'Java APIs. [publisher website]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0201616165&redir=1',
 'title': ['Python Programming with the Java Class Libraries: A Tutorial for '
           'Building Web and Enterprise Applications with Jython']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Chris Fehily; Peachpit Press, 2002, ISBN 0201748843. '
         'Task-based, step-by-step visual reference guide, many screen shots, '
         'for courses in digital graphics; Web design, scripting, development; '
         'multimedia, page layout, office tools, operating systems. [Prentice '
         'Hall]',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0201748843,00%2Ben-USS_01DBC.html',
 'title': ['Python: Visual QuickStart Guide']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Ivan Van Laningham; Sams Publishing, 2000, ISBN 0672317354. '
         'Split into 24 hands-on, 1 hour lessons; steps needed to learn topic: '
         'syntax, language features, OO design and programming, GUIs '
         '(Tkinter), system administration, CGI. [Sams Publishing]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0672317354',
 'title': ['Sams Teach Yourself Python in 24 Hours']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By David Mertz; Addison Wesley. Book in progress, full text, ASCII '
         'format. Asks for feedback. [author website, Gnosis Software, Inc.]',
 'link': 'http://gnosis.cx/TPiP/',
 'title': ['Text Processing in Python']}
2017-08-18 16:20:26 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has '
         'CD-ROM. Methods to build XML applications fast, Python tutorial, DOM '
         'and SAX, new Pyxie open source XML processing library. [Prentice '
         'Hall PTR]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0130211192',
 'title': ['XML Processing with Python']}
2017-08-18 16:20:26 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:20:26 [scrapy.extensions.feedexport] INFO: Stored csv feed (22 items) in: temp.csv
2017-08-18 16:20:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 20, 26, 364209),
 'item_scraped_count': 22,
 'log_count/DEBUG': 28,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'memusage/max': 868106240,
 'memusage/startup': 868106240,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 20, 26, 211418)}
2017-08-18 16:20:26 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:23:00 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:23:00 [scrapy.utils.log] INFO: Overridden settings: {'FEED_FORMAT': 'csv', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'FEED_URI': 'temp.csv', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log'}
2017-08-18 16:23:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-08-18 16:23:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:23:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:23:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:23:00 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:23:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:23:00 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:23:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:23:00 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:23:00 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:23:00 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:23:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:23:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.pearsonhighered.com/educator/academic/product/0,,0130260363,00%2Ben-USS_01DBC.html'],
 'title': ['Core Python Programming']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.brpreiss.com/books/opus7/html/book.html'],
 'title': ['Data Structures and Algorithms with Object-Oriented Design '
           'Patterns in Python']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.diveintopython.net/'],
 'title': ['Dive Into Python 3']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://rhodesmill.org/brandon/2011/foundations-of-python-network-programming/'],
 'title': ['Foundations of Python Network Programming']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.techbooksforfree.com/perlpython.shtml'],
 'title': ['Free Python books']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.freetechbooks.com/python-f6.html'],
 'title': ['FreeTechBooks: Python Scripting Language']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://greenteapress.com/thinkpython/'],
 'title': ['How to Think Like a Computer Scientist: Learning with Python']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.network-theory.co.uk/python/intro/'],
 'title': ['An Introduction to Python']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.freenetpages.co.uk/hp/alan.gauld/'],
 'title': ['Learn to Program Using Python']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471219754.html'],
 'title': ['Making Use of Python']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://hetland.org/writing/practical-python/'],
 'title': ['Practical Python']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://sysadminpy.com/'],
 'title': ['Pro Python System Administration']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.qtrac.eu/py3book.html'],
 'title': ['Programming in Python 3 (Second Edition)']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.wiley.com/WileyCDA/WileyTitle/productCd-0764548077.html'],
 'title': ['Python 2.1 Bible']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['https://www.packtpub.com/python-3-object-oriented-programming/book'],
 'title': ['Python 3 Object Oriented Programming']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.network-theory.co.uk/python/language/'],
 'title': ['Python Language Reference Manual']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.pearsonhighered.com/educator/academic/product/0,,0130409561,00%2Ben-USS_01DBC.html'],
 'title': ['Python Programming Patterns']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.informit.com/store/product.aspx?isbn=0201616165&redir=1'],
 'title': ['Python Programming with the Java Class Libraries: A Tutorial for '
           'Building Web and Enterprise Applications with Jython']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.pearsonhighered.com/educator/academic/product/0,,0201748843,00%2Ben-USS_01DBC.html'],
 'title': ['Python: Visual QuickStart Guide']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.informit.com/store/product.aspx?isbn=0672317354'],
 'title': ['Sams Teach Yourself Python in 24 Hours']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://gnosis.cx/TPiP/'],
 'title': ['Text Processing in Python']}
2017-08-18 16:23:00 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.informit.com/store/product.aspx?isbn=0130211192'],
 'title': ['XML Processing with Python']}
2017-08-18 16:23:00 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:23:00 [scrapy.extensions.feedexport] INFO: Stored csv feed (22 items) in: temp.csv
2017-08-18 16:23:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 23, 0, 424573),
 'item_scraped_count': 22,
 'log_count/DEBUG': 28,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'memusage/max': 869261312,
 'memusage/startup': 869261312,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 23, 0, 269353)}
2017-08-18 16:23:00 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:23:18 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:23:18 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'FEED_URI': 'temp.csv', 'LOG_FILE': 'tutorial.log', 'FEED_FORMAT': 'csv', 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True}
2017-08-18 16:23:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2017-08-18 16:23:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:23:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:23:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:23:19 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:23:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:23:19 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:23:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:23:19 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:23:19 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:23:19 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:23:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:23:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.pearsonhighered.com/educator/academic/product/0,,0130260363,00%2Ben-USS_01DBC.html'],
 'title': ['Core Python Programming']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.brpreiss.com/books/opus7/html/book.html'],
 'title': ['Data Structures and Algorithms with Object-Oriented Design '
           'Patterns in Python']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.diveintopython.net/'],
 'title': ['Dive Into Python 3']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://rhodesmill.org/brandon/2011/foundations-of-python-network-programming/'],
 'title': ['Foundations of Python Network Programming']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.techbooksforfree.com/perlpython.shtml'],
 'title': ['Free Python books']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.freetechbooks.com/python-f6.html'],
 'title': ['FreeTechBooks: Python Scripting Language']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://greenteapress.com/thinkpython/'],
 'title': ['How to Think Like a Computer Scientist: Learning with Python']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.network-theory.co.uk/python/intro/'],
 'title': ['An Introduction to Python']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.freenetpages.co.uk/hp/alan.gauld/'],
 'title': ['Learn to Program Using Python']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471219754.html'],
 'title': ['Making Use of Python']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://hetland.org/writing/practical-python/'],
 'title': ['Practical Python']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://sysadminpy.com/'],
 'title': ['Pro Python System Administration']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.qtrac.eu/py3book.html'],
 'title': ['Programming in Python 3 (Second Edition)']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.wiley.com/WileyCDA/WileyTitle/productCd-0764548077.html'],
 'title': ['Python 2.1 Bible']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['https://www.packtpub.com/python-3-object-oriented-programming/book'],
 'title': ['Python 3 Object Oriented Programming']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.network-theory.co.uk/python/language/'],
 'title': ['Python Language Reference Manual']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.pearsonhighered.com/educator/academic/product/0,,0130409561,00%2Ben-USS_01DBC.html'],
 'title': ['Python Programming Patterns']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.informit.com/store/product.aspx?isbn=0201616165&redir=1'],
 'title': ['Python Programming with the Java Class Libraries: A Tutorial for '
           'Building Web and Enterprise Applications with Jython']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.pearsonhighered.com/educator/academic/product/0,,0201748843,00%2Ben-USS_01DBC.html'],
 'title': ['Python: Visual QuickStart Guide']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.informit.com/store/product.aspx?isbn=0672317354'],
 'title': ['Sams Teach Yourself Python in 24 Hours']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://gnosis.cx/TPiP/'],
 'title': ['Text Processing in Python']}
2017-08-18 16:23:19 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': ['http://www.informit.com/store/product.aspx?isbn=0130211192'],
 'title': ['XML Processing with Python']}
2017-08-18 16:23:19 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:23:19 [scrapy.extensions.feedexport] INFO: Stored csv feed (22 items) in: temp.csv
2017-08-18 16:23:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 23, 19, 162925),
 'item_scraped_count': 22,
 'log_count/DEBUG': 28,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'memusage/max': 869441536,
 'memusage/startup': 869441536,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 23, 19, 9944)}
2017-08-18 16:23:19 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:24:03 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:24:03 [scrapy.utils.log] INFO: Overridden settings: {'FEED_URI': 'temp.csv', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log', 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial', 'FEED_FORMAT': 'csv', 'SPIDER_MODULES': ['tutorial.spiders']}
2017-08-18 16:24:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 16:24:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:24:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:24:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:24:03 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:24:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:24:03 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:24:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:03 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:03 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:24:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130260363,00%2Ben-USS_01DBC.html',
 'title': 'Core Python Programming'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.brpreiss.com/books/opus7/html/book.html',
 'title': 'Data Structures and Algorithms with Object-Oriented Design Patterns '
          'in Python'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.diveintopython.net/',
 'title': 'Dive Into Python 3'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://rhodesmill.org/brandon/2011/foundations-of-python-network-programming/',
 'title': 'Foundations of Python Network Programming'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.techbooksforfree.com/perlpython.shtml',
 'title': 'Free Python books'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.freetechbooks.com/python-f6.html',
 'title': 'FreeTechBooks: Python Scripting Language'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://greenteapress.com/thinkpython/',
 'title': 'How to Think Like a Computer Scientist: Learning with Python'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.network-theory.co.uk/python/intro/',
 'title': 'An Introduction to Python'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.freenetpages.co.uk/hp/alan.gauld/',
 'title': 'Learn to Program Using Python'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471219754.html',
 'title': 'Making Use of Python'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://hetland.org/writing/practical-python/',
 'title': 'Practical Python'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://sysadminpy.com/',
 'title': 'Pro Python System Administration'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.qtrac.eu/py3book.html',
 'title': 'Programming in Python 3 (Second Edition)'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0764548077.html',
 'title': 'Python 2.1 Bible'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'https://www.packtpub.com/python-3-object-oriented-programming/book',
 'title': 'Python 3 Object Oriented Programming'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.network-theory.co.uk/python/language/',
 'title': 'Python Language Reference Manual'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130409561,00%2Ben-USS_01DBC.html',
 'title': 'Python Programming Patterns'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.informit.com/store/product.aspx?isbn=0201616165&redir=1',
 'title': 'Python Programming with the Java Class Libraries: A Tutorial for '
          'Building Web and Enterprise Applications with Jython'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0201748843,00%2Ben-USS_01DBC.html',
 'title': 'Python: Visual QuickStart Guide'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.informit.com/store/product.aspx?isbn=0672317354',
 'title': 'Sams Teach Yourself Python in 24 Hours'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://gnosis.cx/TPiP/',
 'title': 'Text Processing in Python'}
2017-08-18 16:24:04 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.informit.com/store/product.aspx?isbn=0130211192',
 'title': 'XML Processing with Python'}
2017-08-18 16:24:04 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:24:04 [scrapy.extensions.feedexport] INFO: Stored csv feed (22 items) in: temp.csv
2017-08-18 16:24:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 24, 4, 100066),
 'item_scraped_count': 22,
 'log_count/DEBUG': 28,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'memusage/max': 869654528,
 'memusage/startup': 869654528,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 24, 3, 942351)}
2017-08-18 16:24:04 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:24:11 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:24:11 [scrapy.utils.log] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'FEED_FORMAT': 'csv', 'LOG_FILE': 'tutorial.log', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial', 'FEED_URI': 'temp.csv'}
2017-08-18 16:24:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2017-08-18 16:24:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:24:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:24:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:24:11 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:24:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:24:11 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:24:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:11 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:11 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:24:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130260363,00%2Ben-USS_01DBC.html',
 'title': 'Core Python Programming'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.brpreiss.com/books/opus7/html/book.html',
 'title': 'Data Structures and Algorithms with Object-Oriented Design Patterns '
          'in Python'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.diveintopython.net/',
 'title': 'Dive Into Python 3'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://rhodesmill.org/brandon/2011/foundations-of-python-network-programming/',
 'title': 'Foundations of Python Network Programming'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.techbooksforfree.com/perlpython.shtml',
 'title': 'Free Python books'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.freetechbooks.com/python-f6.html',
 'title': 'FreeTechBooks: Python Scripting Language'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://greenteapress.com/thinkpython/',
 'title': 'How to Think Like a Computer Scientist: Learning with Python'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.network-theory.co.uk/python/intro/',
 'title': 'An Introduction to Python'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.freenetpages.co.uk/hp/alan.gauld/',
 'title': 'Learn to Program Using Python'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471219754.html',
 'title': 'Making Use of Python'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://hetland.org/writing/practical-python/',
 'title': 'Practical Python'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://sysadminpy.com/',
 'title': 'Pro Python System Administration'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.qtrac.eu/py3book.html',
 'title': 'Programming in Python 3 (Second Edition)'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0764548077.html',
 'title': 'Python 2.1 Bible'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'https://www.packtpub.com/python-3-object-oriented-programming/book',
 'title': 'Python 3 Object Oriented Programming'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.network-theory.co.uk/python/language/',
 'title': 'Python Language Reference Manual'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130409561,00%2Ben-USS_01DBC.html',
 'title': 'Python Programming Patterns'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.informit.com/store/product.aspx?isbn=0201616165&redir=1',
 'title': 'Python Programming with the Java Class Libraries: A Tutorial for '
          'Building Web and Enterprise Applications with Jython'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0201748843,00%2Ben-USS_01DBC.html',
 'title': 'Python: Visual QuickStart Guide'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.informit.com/store/product.aspx?isbn=0672317354',
 'title': 'Sams Teach Yourself Python in 24 Hours'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://gnosis.cx/TPiP/',
 'title': 'Text Processing in Python'}
2017-08-18 16:24:12 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': [<Selector xpath='text()' data='\r\n\t\r\n                                '>,
          <Selector xpath='text()' data=' \r\n\t\t\t\r\n                                '>,
          <Selector xpath='text()' data='\r\n                                '>],
 'link': 'http://www.informit.com/store/product.aspx?isbn=0130211192',
 'title': 'XML Processing with Python'}
2017-08-18 16:24:12 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:24:12 [scrapy.extensions.feedexport] INFO: Stored csv feed (22 items) in: temp.csv
2017-08-18 16:24:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 24, 12, 69244),
 'item_scraped_count': 22,
 'log_count/DEBUG': 28,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'memusage/max': 869863424,
 'memusage/startup': 869863424,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 24, 11, 914928)}
2017-08-18 16:24:12 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:24:29 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:24:29 [scrapy.utils.log] INFO: Overridden settings: {'FEED_URI': 'temp.csv', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True, 'FEED_FORMAT': 'csv', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'LOG_FILE': 'tutorial.log', 'BOT_NAME': 'tutorial'}
2017-08-18 16:24:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-08-18 16:24:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:24:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:24:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:24:29 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:24:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:24:29 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:24:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:29 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:24:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Wesley J. Chun; Prentice Hall PTR, 2001, ISBN 0130260363. For '
         'experienced developers to improve extant skills; professional level '
         'examples. Starts by introducing syntax, objects, error handling, '
         'functions, classes, built-ins. [Prentice Hall]',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130260363,00%2Ben-USS_01DBC.html',
 'title': 'Core Python Programming'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- The primary goal of this book is to promote object-oriented design '
         'using Python and to illustrate the use of the emerging '
         'object-oriented design patterns.\r\n'
         'A secondary goal of the book is to present mathematical tools just '
         'in time. Analysis techniques and proofs are presented as needed and '
         'in the proper context.',
 'link': 'http://www.brpreiss.com/books/opus7/html/book.html',
 'title': 'Data Structures and Algorithms with Object-Oriented Design Patterns '
          'in Python'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Mark Pilgrim, Guide to Python 3  and its differences from '
         'Python 2. Each chapter starts with a real code sample and explains '
         'it fully. Has a comprehensive appendix of all the syntactic and '
         'semantic changes in Python 3',
 'link': 'http://www.diveintopython.net/',
 'title': 'Dive Into Python 3'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- This book covers a wide range of topics. From raw TCP and UDP to '
         'encryption with TSL, and then to HTTP, SMTP, POP, IMAP, and ssh. It '
         'gives you a good understanding of each field and how to do '
         'everything on the network with Python.',
 'link': 'http://rhodesmill.org/brandon/2011/foundations-of-python-network-programming/',
 'title': 'Foundations of Python Network Programming'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Free Python books and tutorials.',
 'link': 'http://www.techbooksforfree.com/perlpython.shtml',
 'title': 'Free Python books'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Annotated list of free online books on Python scripting language. '
         'Topics range from beginner to advanced.',
 'link': 'http://www.freetechbooks.com/python-f6.html',
 'title': 'FreeTechBooks: Python Scripting Language'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Allen B. Downey, Jeffrey Elkner, Chris Meyers; Green Tea Press, '
         '2002, ISBN 0971677506. Teaches general principles of programming, '
         'via Python as subject language. Thorough, in-depth approach to many '
         'basic and intermediate programming topics. Full text online and '
         'downloads: HTML, PDF, PS, LaTeX. [Free, Green Tea Press]',
 'link': 'http://greenteapress.com/thinkpython/',
 'title': 'How to Think Like a Computer Scientist: Learning with Python'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Guido van Rossum, Fred L. Drake, Jr.; Network Theory Ltd., '
         '2003, ISBN 0954161769. Printed edition of official tutorial, for '
         'v2.x, from Python.org. [Network Theory, online]',
 'link': 'http://www.network-theory.co.uk/python/intro/',
 'title': 'An Introduction to Python'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Book by Alan Gauld with full text online. Introduction for those '
         'learning programming basics: terminology, concepts, methods to write '
         'code. Assumes no prior knowledge but basic computer skills.',
 'link': 'http://www.freenetpages.co.uk/hp/alan.gauld/',
 'title': 'Learn to Program Using Python'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Rashi Gupta; John Wiley and Sons, 2002, ISBN 0471219754. Covers '
         'language basics, use for CGI scripting, GUI development, network '
         'programming; shows why it is one of more sophisticated of popular '
         'scripting languages. [Wiley]',
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471219754.html',
 'title': 'Making Use of Python'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Magnus Lie Hetland; Apress LP, 2002, ISBN 1590590066. Readable '
         'guide to ideas most vital to new users, from basics common to high '
         'level languages, to more specific aspects, to a series of 10 ever '
         'more complex programs. [Apress]',
 'link': 'http://hetland.org/writing/practical-python/',
 'title': 'Practical Python'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Rytis Sileika, ISBN13: 978-1-4302-2605-5, Uses real-world '
         'system administration examples like manage devices with SNMP and '
         'SOAP, build a distributed monitoring system, manage web applications '
         'and parse complex log files, monitor and manage MySQL databases.',
 'link': 'http://sysadminpy.com/',
 'title': 'Pro Python System Administration'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- A Complete Introduction to the Python 3.',
 'link': 'http://www.qtrac.eu/py3book.html',
 'title': 'Programming in Python 3 (Second Edition)'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Dave Brueck, Stephen Tanner; John Wiley and Sons, 2001, ISBN '
         '0764548077. Full coverage, clear explanations, hands-on examples, '
         'full language reference; shows step by step how to use components, '
         'assemble them, form full-featured programs. [John Wiley and Sons]',
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0764548077.html',
 'title': 'Python 2.1 Bible'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- A step-by-step tutorial for OOP in Python 3, including discussion '
         'and examples of abstraction, encapsulation, information hiding, and '
         'raise, handle, define, and manipulate exceptions.',
 'link': 'https://www.packtpub.com/python-3-object-oriented-programming/book',
 'title': 'Python 3 Object Oriented Programming'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Guido van Rossum, Fred L. Drake, Jr.; Network Theory Ltd., '
         '2003, ISBN 0954161785. Printed edition of official language '
         'reference, for v2.x, from Python.org, describes syntax, built-in '
         'datatypes. [Network Theory, online]',
 'link': 'http://www.network-theory.co.uk/python/language/',
 'title': 'Python Language Reference Manual'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Thomas W. Christopher; Prentice Hall PTR, 2002, ISBN '
         '0130409561. Shows how to write large programs, introduces powerful '
         'design patterns that deliver high levels of robustness, scalability, '
         'reuse.',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130409561,00%2Ben-USS_01DBC.html',
 'title': 'Python Programming Patterns'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Richard Hightower; Addison-Wesley, 2002, 0201616165. Begins '
         'with Python basics, many exercises, interactive sessions. Shows '
         'programming novices concepts and practical methods. Shows '
         "programming experts Python's abilities and ways to interface with "
         'Java APIs. [publisher website]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0201616165&redir=1',
 'title': 'Python Programming with the Java Class Libraries: A Tutorial for '
          'Building Web and Enterprise Applications with Jython'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Chris Fehily; Peachpit Press, 2002, ISBN 0201748843. '
         'Task-based, step-by-step visual reference guide, many screen shots, '
         'for courses in digital graphics; Web design, scripting, development; '
         'multimedia, page layout, office tools, operating systems. [Prentice '
         'Hall]',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0201748843,00%2Ben-USS_01DBC.html',
 'title': 'Python: Visual QuickStart Guide'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Ivan Van Laningham; Sams Publishing, 2000, ISBN 0672317354. '
         'Split into 24 hands-on, 1 hour lessons; steps needed to learn topic: '
         'syntax, language features, OO design and programming, GUIs '
         '(Tkinter), system administration, CGI. [Sams Publishing]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0672317354',
 'title': 'Sams Teach Yourself Python in 24 Hours'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By David Mertz; Addison Wesley. Book in progress, full text, ASCII '
         'format. Asks for feedback. [author website, Gnosis Software, Inc.]',
 'link': 'http://gnosis.cx/TPiP/',
 'title': 'Text Processing in Python'}
2017-08-18 16:24:29 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has '
         'CD-ROM. Methods to build XML applications fast, Python tutorial, DOM '
         'and SAX, new Pyxie open source XML processing library. [Prentice '
         'Hall PTR]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0130211192',
 'title': 'XML Processing with Python'}
2017-08-18 16:24:29 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:24:29 [scrapy.extensions.feedexport] INFO: Stored csv feed (22 items) in: temp.csv
2017-08-18 16:24:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 24, 29, 877252),
 'item_scraped_count': 22,
 'log_count/DEBUG': 28,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'memusage/max': 869863424,
 'memusage/startup': 869863424,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 24, 29, 732273)}
2017-08-18 16:24:29 [scrapy.core.engine] INFO: Spider closed (finished)
2017-08-18 16:24:52 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)
2017-08-18 16:24:52 [scrapy.utils.log] INFO: Overridden settings: {'FEED_URI': 'temp.csv', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'FEED_FORMAT': 'csv', 'BOT_NAME': 'tutorial', 'ROBOTSTXT_OBEY': True, 'LOG_FILE': 'tutorial.log', 'SPIDER_MODULES': ['tutorial.spiders']}
2017-08-18 16:24:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.feedexport.FeedExporter']
2017-08-18 16:24:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-08-18 16:24:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-08-18 16:24:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-08-18 16:24:52 [scrapy.core.engine] INFO: Spider opened
2017-08-18 16:24:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-18 16:24:52 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-18 16:24:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:52 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.5/dist-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
FileNotFoundError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-18 16:24:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Books.html> (referer: None)
2017-08-18 16:24:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/Python/tutorial/html/Resources.html> (referer: None)
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Wesley J. Chun; Prentice Hall PTR, 2001, ISBN 0130260363. For '
         'experienced developers to improve extant skills; professional level '
         'examples. Starts by introducing syntax, objects, error handling, '
         'functions, classes, built-ins. [Prentice Hall]',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130260363,00%2Ben-USS_01DBC.html',
 'title': 'Core Python Programming'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- The primary goal of this book is to promote object-oriented design '
         'using Python and to illustrate the use of the emerging '
         'object-oriented design patterns.\r\n'
         'A secondary goal of the book is to present mathematical tools just '
         'in time. Analysis techniques and proofs are presented as needed and '
         'in the proper context.',
 'link': 'http://www.brpreiss.com/books/opus7/html/book.html',
 'title': 'Data Structures and Algorithms with Object-Oriented Design Patterns '
          'in Python'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Mark Pilgrim, Guide to Python 3  and its differences from '
         'Python 2. Each chapter starts with a real code sample and explains '
         'it fully. Has a comprehensive appendix of all the syntactic and '
         'semantic changes in Python 3',
 'link': 'http://www.diveintopython.net/',
 'title': 'Dive Into Python 3'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- This book covers a wide range of topics. From raw TCP and UDP to '
         'encryption with TSL, and then to HTTP, SMTP, POP, IMAP, and ssh. It '
         'gives you a good understanding of each field and how to do '
         'everything on the network with Python.',
 'link': 'http://rhodesmill.org/brandon/2011/foundations-of-python-network-programming/',
 'title': 'Foundations of Python Network Programming'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Free Python books and tutorials.',
 'link': 'http://www.techbooksforfree.com/perlpython.shtml',
 'title': 'Free Python books'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Annotated list of free online books on Python scripting language. '
         'Topics range from beginner to advanced.',
 'link': 'http://www.freetechbooks.com/python-f6.html',
 'title': 'FreeTechBooks: Python Scripting Language'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Allen B. Downey, Jeffrey Elkner, Chris Meyers; Green Tea Press, '
         '2002, ISBN 0971677506. Teaches general principles of programming, '
         'via Python as subject language. Thorough, in-depth approach to many '
         'basic and intermediate programming topics. Full text online and '
         'downloads: HTML, PDF, PS, LaTeX. [Free, Green Tea Press]',
 'link': 'http://greenteapress.com/thinkpython/',
 'title': 'How to Think Like a Computer Scientist: Learning with Python'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Guido van Rossum, Fred L. Drake, Jr.; Network Theory Ltd., '
         '2003, ISBN 0954161769. Printed edition of official tutorial, for '
         'v2.x, from Python.org. [Network Theory, online]',
 'link': 'http://www.network-theory.co.uk/python/intro/',
 'title': 'An Introduction to Python'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- Book by Alan Gauld with full text online. Introduction for those '
         'learning programming basics: terminology, concepts, methods to write '
         'code. Assumes no prior knowledge but basic computer skills.',
 'link': 'http://www.freenetpages.co.uk/hp/alan.gauld/',
 'title': 'Learn to Program Using Python'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Rashi Gupta; John Wiley and Sons, 2002, ISBN 0471219754. Covers '
         'language basics, use for CGI scripting, GUI development, network '
         'programming; shows why it is one of more sophisticated of popular '
         'scripting languages. [Wiley]',
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471219754.html',
 'title': 'Making Use of Python'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Magnus Lie Hetland; Apress LP, 2002, ISBN 1590590066. Readable '
         'guide to ideas most vital to new users, from basics common to high '
         'level languages, to more specific aspects, to a series of 10 ever '
         'more complex programs. [Apress]',
 'link': 'http://hetland.org/writing/practical-python/',
 'title': 'Practical Python'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Rytis Sileika, ISBN13: 978-1-4302-2605-5, Uses real-world '
         'system administration examples like manage devices with SNMP and '
         'SOAP, build a distributed monitoring system, manage web applications '
         'and parse complex log files, monitor and manage MySQL databases.',
 'link': 'http://sysadminpy.com/',
 'title': 'Pro Python System Administration'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- A Complete Introduction to the Python 3.',
 'link': 'http://www.qtrac.eu/py3book.html',
 'title': 'Programming in Python 3 (Second Edition)'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Dave Brueck, Stephen Tanner; John Wiley and Sons, 2001, ISBN '
         '0764548077. Full coverage, clear explanations, hands-on examples, '
         'full language reference; shows step by step how to use components, '
         'assemble them, form full-featured programs. [John Wiley and Sons]',
 'link': 'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0764548077.html',
 'title': 'Python 2.1 Bible'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- A step-by-step tutorial for OOP in Python 3, including discussion '
         'and examples of abstraction, encapsulation, information hiding, and '
         'raise, handle, define, and manipulate exceptions.',
 'link': 'https://www.packtpub.com/python-3-object-oriented-programming/book',
 'title': 'Python 3 Object Oriented Programming'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Guido van Rossum, Fred L. Drake, Jr.; Network Theory Ltd., '
         '2003, ISBN 0954161785. Printed edition of official language '
         'reference, for v2.x, from Python.org, describes syntax, built-in '
         'datatypes. [Network Theory, online]',
 'link': 'http://www.network-theory.co.uk/python/language/',
 'title': 'Python Language Reference Manual'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Thomas W. Christopher; Prentice Hall PTR, 2002, ISBN '
         '0130409561. Shows how to write large programs, introduces powerful '
         'design patterns that deliver high levels of robustness, scalability, '
         'reuse.',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0130409561,00%2Ben-USS_01DBC.html',
 'title': 'Python Programming Patterns'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Richard Hightower; Addison-Wesley, 2002, 0201616165. Begins '
         'with Python basics, many exercises, interactive sessions. Shows '
         'programming novices concepts and practical methods. Shows '
         "programming experts Python's abilities and ways to interface with "
         'Java APIs. [publisher website]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0201616165&redir=1',
 'title': 'Python Programming with the Java Class Libraries: A Tutorial for '
          'Building Web and Enterprise Applications with Jython'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Chris Fehily; Peachpit Press, 2002, ISBN 0201748843. '
         'Task-based, step-by-step visual reference guide, many screen shots, '
         'for courses in digital graphics; Web design, scripting, development; '
         'multimedia, page layout, office tools, operating systems. [Prentice '
         'Hall]',
 'link': 'http://www.pearsonhighered.com/educator/academic/product/0,,0201748843,00%2Ben-USS_01DBC.html',
 'title': 'Python: Visual QuickStart Guide'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Ivan Van Laningham; Sams Publishing, 2000, ISBN 0672317354. '
         'Split into 24 hands-on, 1 hour lessons; steps needed to learn topic: '
         'syntax, language features, OO design and programming, GUIs '
         '(Tkinter), system administration, CGI. [Sams Publishing]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0672317354',
 'title': 'Sams Teach Yourself Python in 24 Hours'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By David Mertz; Addison Wesley. Book in progress, full text, ASCII '
         'format. Asks for feedback. [author website, Gnosis Software, Inc.]',
 'link': 'http://gnosis.cx/TPiP/',
 'title': 'Text Processing in Python'}
2017-08-18 16:24:52 [scrapy.core.scraper] DEBUG: Scraped from <200 file:///home/lwq/Desktop/Python/tutorial/html/Books.html>
{'desc': '- By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has '
         'CD-ROM. Methods to build XML applications fast, Python tutorial, DOM '
         'and SAX, new Pyxie open source XML processing library. [Prentice '
         'Hall PTR]',
 'link': 'http://www.informit.com/store/product.aspx?isbn=0130211192',
 'title': 'XML Processing with Python'}
2017-08-18 16:24:52 [scrapy.core.engine] INFO: Closing spider (finished)
2017-08-18 16:24:52 [scrapy.extensions.feedexport] INFO: Stored csv feed (22 items) in: temp.csv
2017-08-18 16:24:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/builtins.FileNotFoundError': 3,
 'downloader/request_bytes': 1120,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 49580,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 18, 8, 24, 52, 810322),
 'item_scraped_count': 22,
 'log_count/DEBUG': 28,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'memusage/max': 869863424,
 'memusage/startup': 869863424,
 'response_received_count': 2,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/builtins.FileNotFoundError': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 8, 18, 8, 24, 52, 657239)}
2017-08-18 16:24:52 [scrapy.core.engine] INFO: Spider closed (finished)
