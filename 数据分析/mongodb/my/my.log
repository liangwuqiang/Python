2017-08-17 14:29:35 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:29:35 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:29:35 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:29:35 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:29:35 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:29:36 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:29:36 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:29:36 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:29:36 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:29:36 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:29:36 [scrapy] INFO: Spider opened
2017-08-17 14:29:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:29:36 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:29:36 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:29:36 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:29:36 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:29:36 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:29:36 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:29:36 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:29:36 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 29, 36, 941083),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 29, 36, 817968)}
2017-08-17 14:29:36 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:33:49 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:33:49 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:33:49 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:33:49 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:33:49 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:33:50 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:33:50 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:33:50 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:33:50 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:33:50 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:33:50 [scrapy] INFO: Spider opened
2017-08-17 14:33:50 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:33:50 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:33:50 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:33:50 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:33:50 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:33:50 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:33:50 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:33:51 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:33:51 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 33, 51, 53649),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 33, 50, 938282)}
2017-08-17 14:33:51 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:49:15 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:49:15 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:49:15 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:49:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:49:16 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:49:17 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:49:17 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:49:17 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:49:17 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:49:17 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:49:17 [scrapy] INFO: Spider opened
2017-08-17 14:49:17 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:49:17 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:49:17 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:49:17 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:49:17 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:49:17 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:49:17 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:49:17 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:49:17 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 49, 17, 175129),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 49, 17, 47883)}
2017-08-17 14:49:17 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:49:46 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:49:46 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:49:46 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:49:46 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:49:46 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:49:47 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:49:47 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:49:47 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:49:47 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:49:47 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:49:47 [scrapy] INFO: Spider opened
2017-08-17 14:49:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:49:47 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:49:47 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:49:47 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:49:47 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:49:47 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:49:47 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:49:47 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:49:47 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 49, 47, 621858),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 49, 47, 495702)}
2017-08-17 14:49:47 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:50:32 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:50:32 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:50:32 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:50:32 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:50:32 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:50:33 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:50:33 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:50:33 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:50:33 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:50:33 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:50:33 [scrapy] INFO: Spider opened
2017-08-17 14:50:33 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:50:33 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:50:33 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:50:33 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:50:33 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:50:33 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:50:33 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:50:33 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 16, in parse
    sel = selector.Selector(response).decode('utf-8')
AttributeError: 'Selector' object has no attribute 'decode'
2017-08-17 14:50:33 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:50:33 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 50, 33, 465373),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 50, 33, 244543)}
2017-08-17 14:50:33 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:50:52 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:50:52 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:50:52 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:50:52 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:50:52 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:50:53 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:50:53 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:50:53 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:50:53 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:50:53 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:50:53 [scrapy] INFO: Spider opened
2017-08-17 14:50:53 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:50:53 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:50:53 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:50:53 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:50:53 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:50:53 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:50:53 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:50:53 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:50:53 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 50, 53, 701947),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 50, 53, 576517)}
2017-08-17 14:50:53 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:51:54 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:51:54 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:51:54 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:51:54 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:51:54 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:51:55 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:51:55 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:51:55 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:51:55 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:51:55 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:51:55 [scrapy] INFO: Spider opened
2017-08-17 14:51:55 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:51:55 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:51:55 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:51:55 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:51:55 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:51:55 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:51:55 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:51:55 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:51:55 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 51, 55, 687668),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 51, 55, 560885)}
2017-08-17 14:51:55 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:52:36 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:52:36 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:52:36 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:52:36 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:52:36 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:52:37 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:52:37 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:52:37 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:52:37 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:52:37 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:52:37 [scrapy] INFO: Spider opened
2017-08-17 14:52:37 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:52:37 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:52:37 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:52:37 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:52:37 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:52:37 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:52:37 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:52:37 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:52:37 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 52, 37, 350991),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 52, 37, 224356)}
2017-08-17 14:52:37 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:53:59 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:53:59 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:53:59 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:53:59 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:54:00 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:54:01 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:54:01 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:54:01 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:54:01 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:54:01 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:54:01 [scrapy] INFO: Spider opened
2017-08-17 14:54:01 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:54:01 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:54:01 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:54:01 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:54:01 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:54:01 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:54:01 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:54:01 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:54:01 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 54, 1, 178528),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 54, 1, 52901)}
2017-08-17 14:54:01 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:54:26 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:54:26 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:54:26 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:54:26 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:54:26 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:54:27 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:54:27 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:54:27 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:54:27 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:54:27 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:54:27 [scrapy] INFO: Spider opened
2017-08-17 14:54:27 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:54:27 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:54:27 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:54:27 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:54:27 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:54:27 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:54:27 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:54:28 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:54:28 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 54, 28, 88247),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 54, 27, 956401)}
2017-08-17 14:54:28 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:59:11 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:59:11 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:59:11 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:59:11 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:59:11 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:59:12 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:59:12 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:59:12 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:59:12 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:59:12 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:59:12 [scrapy] INFO: Spider opened
2017-08-17 14:59:12 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:59:12 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:59:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:12 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:12 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:12 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:59:13 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:59:13 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 59, 13, 29966),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 59, 12, 902417)}
2017-08-17 14:59:13 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:59:31 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:59:31 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:59:31 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:59:31 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:59:31 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:59:32 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:59:32 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:59:32 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:59:32 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:59:32 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:59:32 [scrapy] INFO: Spider opened
2017-08-17 14:59:32 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:59:32 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:59:32 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:32 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:32 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:32 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:32 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:59:32 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:59:32 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 59, 32, 680384),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 59, 32, 553892)}
2017-08-17 14:59:32 [scrapy] INFO: Spider closed (finished)
2017-08-17 14:59:46 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 14:59:46 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 14:59:46 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 14:59:46 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 14:59:46 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 14:59:47 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 14:59:47 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 14:59:47 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 14:59:47 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 14:59:47 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 14:59:47 [scrapy] INFO: Spider opened
2017-08-17 14:59:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 14:59:47 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 14:59:47 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:47 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:47 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:47 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 14:59:47 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 14:59:48 [scrapy] INFO: Closing spider (finished)
2017-08-17 14:59:48 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 6, 59, 48, 80078),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 6, 59, 47, 947194)}
2017-08-17 14:59:48 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:00:06 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:00:06 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:00:06 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:00:06 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:00:06 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:00:07 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:00:07 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:00:07 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:00:07 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:00:07 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:00:07 [scrapy] INFO: Spider opened
2017-08-17 15:00:07 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:00:07 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:00:07 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:00:07 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:00:07 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:00:07 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:00:07 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:00:07 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:00:07 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 0, 7, 774101),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 0, 7, 646742)}
2017-08-17 15:00:07 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:01:19 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:01:19 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:01:19 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:01:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:01:19 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:01:20 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:01:20 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:01:20 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:01:20 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:01:20 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:01:20 [scrapy] INFO: Spider opened
2017-08-17 15:01:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:01:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:01:20 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:01:20 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:01:20 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:01:20 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:01:20 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:01:21 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:01:21 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 1, 21, 40636),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 1, 20, 915330)}
2017-08-17 15:01:21 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:02:17 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:02:17 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:02:17 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:02:17 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:02:17 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:02:18 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:02:18 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:02:18 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:02:18 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:02:18 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:02:18 [scrapy] INFO: Spider opened
2017-08-17 15:02:18 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:02:18 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:02:18 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:02:18 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:02:18 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:02:18 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:02:18 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:02:19 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:02:19 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 2, 19, 89930),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 2, 18, 963486)}
2017-08-17 15:02:19 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:02:34 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:02:34 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:02:34 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:02:34 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:02:34 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:02:35 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:02:35 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:02:35 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:02:35 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:02:35 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:02:35 [scrapy] INFO: Spider opened
2017-08-17 15:02:35 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:02:35 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:02:35 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:02:35 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:02:35 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:02:35 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:02:35 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:02:35 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:02:35 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 2, 35, 310669),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 2, 35, 186824)}
2017-08-17 15:02:35 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:04:11 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:04:11 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:04:11 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:04:11 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:04:11 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:04:12 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:04:12 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:04:12 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:04:12 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:04:12 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:04:12 [scrapy] INFO: Spider opened
2017-08-17 15:04:12 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:04:12 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:04:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:04:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:04:12 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:04:12 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:04:12 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:04:12 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 23, in parse
    print(item['title'].decode('utf-8'))
  File "/usr/lib/python2.7/encodings/utf_8.py", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeEncodeError: 'ascii' codec can't encode characters in position 8-9: ordinal not in range(128)
2017-08-17 15:04:12 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:04:12 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 4, 12, 719937),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 4, 12, 497528)}
2017-08-17 15:04:12 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:04:28 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:04:28 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:04:28 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:04:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:04:28 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:04:29 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:04:29 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:04:29 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:04:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:04:29 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:04:29 [scrapy] INFO: Spider opened
2017-08-17 15:04:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:04:29 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:04:29 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:04:29 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:04:29 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:04:29 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:04:29 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:04:29 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:04:29 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 4, 29, 497947),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 4, 29, 372725)}
2017-08-17 15:04:29 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:07:18 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:07:18 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:07:18 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:07:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:07:19 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:07:20 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:07:20 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:07:20 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:07:20 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:07:20 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:07:20 [scrapy] INFO: Spider opened
2017-08-17 15:07:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:07:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:07:20 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:07:20 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:07:20 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:07:20 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:07:20 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:07:20 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 26, in parse
    f.write(item)
TypeError: expected a string or other character buffer object
2017-08-17 15:07:20 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:07:20 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 7, 20, 391103),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 7, 20, 166736)}
2017-08-17 15:07:20 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:09:19 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:09:19 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:09:19 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:09:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:09:19 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:09:20 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:09:20 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:09:20 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:09:20 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:09:20 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:09:20 [scrapy] INFO: Spider opened
2017-08-17 15:09:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:09:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:09:20 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:09:20 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:09:20 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:09:20 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:09:20 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:09:20 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 26, in parse
    f.write(item['title'])
UnicodeEncodeError: 'ascii' codec can't encode characters in position 8-9: ordinal not in range(128)
2017-08-17 15:09:20 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:09:20 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 9, 20, 648225),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 9, 20, 423669)}
2017-08-17 15:09:20 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:09:59 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:09:59 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:09:59 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:09:59 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:09:59 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:10:00 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:10:00 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:10:00 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:10:00 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:10:00 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:10:00 [scrapy] INFO: Spider opened
2017-08-17 15:10:00 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:10:00 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:10:00 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:00 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:00 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:00 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:00 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:10:00 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 26, in parse
    f.write(item['title'].decode('utf-8'))
  File "/usr/lib/python2.7/encodings/utf_8.py", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeEncodeError: 'ascii' codec can't encode characters in position 8-9: ordinal not in range(128)
2017-08-17 15:10:01 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:10:01 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 10, 1, 58286),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 10, 0, 831959)}
2017-08-17 15:10:01 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:10:29 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:10:29 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:10:29 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:10:29 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:10:29 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:10:30 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:10:30 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:10:30 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:10:30 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:10:30 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:10:30 [scrapy] INFO: Spider opened
2017-08-17 15:10:30 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:10:30 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:10:30 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:30 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:30 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:30 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:30 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:10:30 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:10:30 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 10, 30, 492034),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 10, 30, 370375)}
2017-08-17 15:10:30 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:10:54 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:10:54 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:10:54 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:10:55 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:10:55 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:10:56 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:10:56 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:10:56 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:10:56 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:10:56 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:10:56 [scrapy] INFO: Spider opened
2017-08-17 15:10:56 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:10:56 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:10:56 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:56 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:56 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:56 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:10:56 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:10:56 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:10:56 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 10, 56, 231608),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 10, 56, 103487)}
2017-08-17 15:10:56 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:11:33 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:11:33 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:11:33 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:11:33 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:11:33 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:11:34 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:11:34 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:11:34 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:11:34 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:11:34 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:11:34 [scrapy] INFO: Spider opened
2017-08-17 15:11:34 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:11:34 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:11:34 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:11:34 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:11:34 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:11:34 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:11:34 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:11:34 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 26, in parse
    f.write(item.encode('utf-8'))
  File "/usr/lib/python2.7/dist-packages/scrapy/item.py", line 71, in __getattr__
    raise AttributeError(name)
AttributeError: encode
2017-08-17 15:11:34 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:11:34 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 11, 34, 819942),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 11, 34, 593869)}
2017-08-17 15:11:34 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:16:16 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:16:16 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:16:16 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:16:17 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:16:17 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:16:18 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:16:18 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:16:18 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:16:18 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:16:18 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:16:18 [scrapy] INFO: Spider opened
2017-08-17 15:16:18 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:16:18 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:16:18 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:16:18 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:16:18 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:16:18 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:16:18 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:16:18 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:16:18 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 16, 18, 286751),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 16, 18, 154839)}
2017-08-17 15:16:18 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:27:11 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:27:11 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:27:11 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:27:11 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:27:11 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:27:12 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:27:12 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:27:12 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:27:12 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:27:12 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:27:12 [scrapy] INFO: Spider opened
2017-08-17 15:27:12 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:27:12 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:27:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:27:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:27:12 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:27:12 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:27:12 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:27:12 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:27:12 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 27, 12, 486231),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 27, 12, 350650)}
2017-08-17 15:27:12 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:27:42 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:27:42 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:27:42 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:27:42 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:27:42 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:27:43 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:27:43 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:27:43 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:27:43 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:27:43 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:27:43 [scrapy] INFO: Spider opened
2017-08-17 15:27:43 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:27:43 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:27:43 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:27:43 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:27:43 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:27:43 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:27:43 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:27:44 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:27:44 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 27, 44, 7206),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 27, 43, 879712)}
2017-08-17 15:27:44 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:32:23 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:32:23 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:32:23 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:32:23 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:32:24 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:32:25 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:32:25 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:32:25 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:32:25 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:32:25 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:32:25 [scrapy] INFO: Spider opened
2017-08-17 15:32:25 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:32:25 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:32:25 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:32:25 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:32:25 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:32:25 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:32:25 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:32:25 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): www.runoob.com
2017-08-17 15:32:25 [urllib3.connectionpool] DEBUG: http://www.runoob.com:80 "GET /mongodb/mongodb-tutorial.html HTTP/1.1" 200 15176
2017-08-17 15:32:26 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 27, in parse
    self.get_web(item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 33, in get_web
    f.write(html)
UnicodeEncodeError: 'ascii' codec can't encode characters in position 257-258: ordinal not in range(128)
2017-08-17 15:32:27 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:32:27 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 32, 27, 17787),
 'log_count/DEBUG': 8,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 32, 25, 41508)}
2017-08-17 15:32:27 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:34:10 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:34:10 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:34:10 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:34:10 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:34:10 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:34:11 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:34:11 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:34:11 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:34:11 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:34:11 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:34:11 [scrapy] INFO: Spider opened
2017-08-17 15:34:11 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:34:11 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:34:11 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:34:11 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:34:11 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:34:11 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:34:11 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:34:12 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): www.runoob.com
2017-08-17 15:34:12 [urllib3.connectionpool] DEBUG: http://www.runoob.com:80 "GET /mongodb/mongodb-tutorial.html HTTP/1.1" 200 15176
2017-08-17 15:34:14 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 27, in parse
    self.get_web(item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 33, in get_web
    f.write(html)
UnicodeEncodeError: 'ascii' codec can't encode characters in position 257-258: ordinal not in range(128)
2017-08-17 15:34:14 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:34:14 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 34, 14, 267917),
 'log_count/DEBUG': 8,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 34, 11, 889471)}
2017-08-17 15:34:14 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:35:12 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:35:12 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:35:12 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:35:12 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:35:12 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:35:13 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:35:13 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:35:13 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:35:13 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:35:13 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:35:13 [scrapy] INFO: Spider opened
2017-08-17 15:35:13 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:35:13 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:35:13 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:13 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:13 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:13 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:13 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:35:13 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:35:13 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 35, 13, 661709),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 35, 13, 538744)}
2017-08-17 15:35:13 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:35:28 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:35:28 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:35:28 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:35:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:35:29 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:35:30 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:35:30 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:35:30 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:35:30 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:35:30 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:35:30 [scrapy] INFO: Spider opened
2017-08-17 15:35:30 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:35:30 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:35:30 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:30 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:30 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:30 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:30 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:35:30 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:35:30 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 35, 30, 156080),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 35, 30, 30453)}
2017-08-17 15:35:30 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:35:40 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:35:40 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:35:40 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:35:41 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:35:41 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:35:42 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:35:42 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:35:42 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:35:42 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:35:42 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:35:42 [scrapy] INFO: Spider opened
2017-08-17 15:35:42 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:35:42 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:35:42 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:42 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:42 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:42 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:35:42 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:35:42 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:35:42 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 35, 42, 290326),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 35, 42, 165902)}
2017-08-17 15:35:42 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:36:08 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:36:08 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:36:08 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:36:09 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:36:09 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:36:10 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:36:10 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:36:10 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:36:10 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:36:10 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:36:10 [scrapy] INFO: Spider opened
2017-08-17 15:36:10 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:36:10 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:36:10 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:36:10 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:36:10 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:36:10 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:36:10 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:36:10 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): www.runoob.com
2017-08-17 15:36:10 [urllib3.connectionpool] DEBUG: http://www.runoob.com:80 "GET /mongodb/mongodb-tutorial.html HTTP/1.1" 200 15176
2017-08-17 15:36:16 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 27, in parse
    self.get_web(item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 34, in get_web
    f.write(html)
UnicodeEncodeError: 'ascii' codec can't encode characters in position 257-258: ordinal not in range(128)
2017-08-17 15:36:16 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:36:16 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 36, 16, 651917),
 'log_count/DEBUG': 8,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 36, 10, 133774)}
2017-08-17 15:36:16 [scrapy] INFO: Spider closed (finished)
2017-08-17 15:36:52 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 15:36:52 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 15:36:52 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 15:36:52 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 15:36:52 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 15:36:53 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 15:36:53 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 15:36:53 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 15:36:53 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 15:36:53 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 15:36:53 [scrapy] INFO: Spider opened
2017-08-17 15:36:53 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 15:36:53 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 15:36:53 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:36:53 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:36:53 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:36:53 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 15:36:53 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 15:36:53 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): www.runoob.com
2017-08-17 15:36:53 [urllib3.connectionpool] DEBUG: http://www.runoob.com:80 "GET /mongodb/mongodb-tutorial.html HTTP/1.1" 200 15176
2017-08-17 15:37:09 [scrapy] INFO: Closing spider (finished)
2017-08-17 15:37:09 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 7, 37, 9, 470457),
 'log_count/DEBUG': 8,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 7, 36, 53, 375376)}
2017-08-17 15:37:09 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:04:44 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:04:44 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:04:44 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:04:44 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:04:44 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:04:45 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:04:45 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:04:45 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:04:45 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:04:45 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:04:45 [scrapy] INFO: Spider opened
2017-08-17 16:04:45 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:04:45 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:04:45 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:04:45 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:04:45 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:04:45 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:04:45 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:04:46 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:04:46 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 4, 46, 98541),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 4, 45, 972114)}
2017-08-17 16:04:46 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:06:18 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:06:18 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:06:18 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:06:18 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:06:18 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:06:19 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:06:19 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:06:19 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:06:19 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:06:19 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:06:19 [scrapy] INFO: Spider opened
2017-08-17 16:06:19 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:06:19 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:06:19 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:06:19 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:06:19 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:06:19 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:06:19 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:06:19 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:06:19 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 6, 19, 686763),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 6, 19, 563431)}
2017-08-17 16:06:19 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:07:47 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:07:47 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:07:47 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:07:47 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:07:48 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:07:49 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:07:49 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:07:49 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:07:49 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:07:49 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:07:49 [scrapy] INFO: Spider opened
2017-08-17 16:07:49 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:07:49 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:07:49 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:07:49 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:07:49 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:07:49 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:07:49 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:07:49 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:07:49 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 7, 49, 214106),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 7, 49, 35085)}
2017-08-17 16:07:49 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:29:16 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:29:16 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:29:16 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:29:16 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:29:16 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:29:17 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:29:17 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:29:17 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:29:17 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:29:17 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:29:17 [scrapy] INFO: Spider opened
2017-08-17 16:29:17 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:29:17 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:29:17 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:29:17 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:29:17 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:29:17 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:29:17 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:29:20 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:29:20 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 29, 20, 641196),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 29, 17, 799969)}
2017-08-17 16:29:20 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:35:50 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:35:50 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:35:50 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:35:50 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:35:50 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:35:51 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:35:51 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:35:51 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:35:51 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:35:51 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:35:51 [scrapy] INFO: Spider opened
2017-08-17 16:35:51 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:35:51 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:35:51 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:35:51 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:35:51 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:35:51 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:35:51 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:35:54 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:35:54 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 35, 54, 305678),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 35, 51, 639232)}
2017-08-17 16:35:54 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:36:31 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:36:31 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:36:31 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:36:31 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:36:31 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:36:32 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:36:32 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:36:32 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:36:32 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:36:32 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:36:32 [scrapy] INFO: Spider opened
2017-08-17 16:36:32 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:36:32 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:36:32 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:36:32 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:36:32 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:36:32 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:36:33 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:36:33 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:36:33 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 36, 33, 178744),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 36, 32, 993616)}
2017-08-17 16:36:33 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:36:45 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:36:45 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:36:45 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:36:45 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:36:45 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:36:46 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:36:46 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:36:46 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:36:46 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:36:46 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:36:46 [scrapy] INFO: Spider opened
2017-08-17 16:36:46 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:36:46 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:36:46 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:36:46 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:36:46 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:36:46 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:36:46 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:36:47 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:36:47 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 36, 47, 36732),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 36, 46, 849392)}
2017-08-17 16:36:47 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:37:17 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:37:17 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:37:17 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:37:17 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:37:17 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:37:18 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:37:18 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:37:18 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:37:18 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:37:18 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:37:18 [scrapy] INFO: Spider opened
2017-08-17 16:37:18 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:37:18 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:37:18 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:37:18 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:37:18 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:37:18 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:37:18 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:37:18 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:37:18 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 37, 18, 669348),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 37, 18, 482269)}
2017-08-17 16:37:18 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:37:44 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:37:44 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:37:44 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:37:44 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:37:44 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:37:45 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:37:45 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:37:45 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:37:45 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:37:45 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:37:45 [scrapy] INFO: Spider opened
2017-08-17 16:37:45 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:37:45 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:37:45 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:37:45 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:37:45 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:37:45 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:37:45 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:37:45 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:37:45 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 37, 45, 629341),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 37, 45, 438689)}
2017-08-17 16:37:45 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:38:47 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:38:47 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:38:47 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:38:47 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:38:47 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:38:48 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:38:48 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:38:48 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:38:48 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:38:48 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:38:48 [scrapy] INFO: Spider opened
2017-08-17 16:38:48 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:38:48 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:38:48 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:38:48 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:38:48 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:38:48 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:38:48 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:38:49 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:38:49 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 38, 49, 157101),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 38, 48, 970520)}
2017-08-17 16:38:49 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:40:40 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:40:40 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:40:40 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:40:40 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:40:40 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:40:41 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:40:41 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:40:41 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:40:41 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:40:41 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:40:41 [scrapy] INFO: Spider opened
2017-08-17 16:40:41 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:40:41 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:40:41 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:40:41 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:40:41 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:40:41 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:40:41 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:40:41 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 31, in parse
    self.get_web(index, item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 54, in get_web
    with open(imagename) as f:
IOError: [Errno 2] No such file or directory: 'output/images/17dd725b73cb0b6b7288b71e9bbc52a1.png'
2017-08-17 16:40:41 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:40:41 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 40, 41, 996597),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IOError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 40, 41, 716378)}
2017-08-17 16:40:41 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:42:30 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:42:30 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:42:30 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:42:30 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:42:30 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:42:31 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:42:31 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:42:31 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:42:31 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:42:31 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:42:31 [scrapy] INFO: Spider opened
2017-08-17 16:42:31 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:42:31 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:42:31 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:42:31 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:42:31 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:42:31 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:42:31 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:42:31 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 31, in parse
    self.get_web(index, item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 54, in get_web
    with open(imagename) as f:
IOError: [Errno 2] No such file or directory: 'output/images/17dd725b73cb0b6b7288b71e9bbc52a1.png'
2017-08-17 16:42:31 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:42:31 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 42, 31, 815353),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IOError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 42, 31, 535312)}
2017-08-17 16:42:31 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:42:38 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:42:38 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:42:38 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:42:38 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:42:38 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:42:39 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:42:39 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:42:39 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:42:39 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:42:39 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:42:39 [scrapy] INFO: Spider opened
2017-08-17 16:42:39 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:42:39 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:42:39 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:42:39 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:42:39 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:42:39 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:42:39 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:42:40 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:42:40 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 42, 40, 25362),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 42, 39, 838171)}
2017-08-17 16:42:40 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:43:51 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:43:51 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:43:51 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:43:51 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:43:51 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:43:52 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:43:52 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:43:52 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:43:52 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:43:52 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:43:52 [scrapy] INFO: Spider opened
2017-08-17 16:43:52 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:43:52 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:43:52 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:43:52 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:43:52 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:43:52 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:43:52 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:43:53 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:43:53 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 43, 53, 57341),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 43, 52, 871145)}
2017-08-17 16:43:53 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:44:23 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:44:23 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:44:23 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:44:23 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:44:23 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:44:24 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:44:24 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:44:24 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:44:24 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:44:24 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:44:24 [scrapy] INFO: Spider opened
2017-08-17 16:44:24 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:44:24 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:44:24 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:44:24 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:44:24 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:44:24 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:44:24 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:44:25 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 31, in parse
    self.get_web(index, item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 55, in get_web
    with open(imagename) as f:
IOError: [Errno 2] No such file or directory: '17dd725b73cb0b6b7288b71e9bbc52a1.png'
2017-08-17 16:44:25 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:44:25 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 44, 25, 243940),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IOError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 44, 24, 959812)}
2017-08-17 16:44:25 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:44:55 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:44:55 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:44:55 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:44:55 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:44:55 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:44:56 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:44:56 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:44:56 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:44:56 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:44:56 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:44:56 [scrapy] INFO: Spider opened
2017-08-17 16:44:56 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:44:56 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:44:56 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:44:56 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:44:56 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:44:56 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:44:56 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:44:57 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:44:57 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 44, 57, 94857),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 44, 56, 914925)}
2017-08-17 16:44:57 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:45:45 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:45:45 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:45:45 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:45:45 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:45:45 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:45:46 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:45:46 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:45:46 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:45:46 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:45:46 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:45:46 [scrapy] INFO: Spider opened
2017-08-17 16:45:46 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:45:46 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:45:46 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:45:46 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:45:46 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:45:46 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:45:46 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:45:46 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:45:46 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 45, 46, 693822),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 45, 46, 505592)}
2017-08-17 16:45:46 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:46:29 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:46:29 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:46:29 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:46:29 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:46:29 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:46:30 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:46:30 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:46:30 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:46:30 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:46:30 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:46:30 [scrapy] INFO: Spider opened
2017-08-17 16:46:30 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:46:30 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:46:30 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:46:30 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:46:30 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:46:30 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:46:30 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:46:30 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:46:30 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 46, 30, 732929),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 46, 30, 549569)}
2017-08-17 16:46:30 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:49:04 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:49:04 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:49:04 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:49:04 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:49:04 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:49:05 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:49:05 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:49:05 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:49:05 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:49:05 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:49:05 [scrapy] INFO: Spider opened
2017-08-17 16:49:05 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:49:05 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:49:05 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:49:05 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:49:05 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:49:05 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:49:05 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:49:06 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:49:06 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 49, 6, 114979),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 49, 5, 927714)}
2017-08-17 16:49:06 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:54:09 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:54:09 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:54:09 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:54:09 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:54:09 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:54:10 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:54:10 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:54:10 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:54:10 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:54:10 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:54:10 [scrapy] INFO: Spider opened
2017-08-17 16:54:10 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:54:10 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:54:10 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:54:10 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:54:10 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:54:10 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:54:10 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:54:10 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:54:10 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 54, 10, 623146),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 54, 10, 420550)}
2017-08-17 16:54:10 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:54:46 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:54:46 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:54:46 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:54:47 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:54:47 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:54:48 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:54:48 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:54:48 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:54:48 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:54:48 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:54:48 [scrapy] INFO: Spider opened
2017-08-17 16:54:48 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:54:48 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:54:48 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:54:48 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:54:48 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:54:48 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:54:48 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:54:48 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:54:48 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 54, 48, 506948),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 54, 48, 296062)}
2017-08-17 16:54:48 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:55:19 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:55:19 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:55:19 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:55:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:55:19 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:55:20 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:55:20 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:55:20 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:55:20 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:55:20 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:55:20 [scrapy] INFO: Spider opened
2017-08-17 16:55:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:55:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:55:20 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:55:20 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:55:20 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:55:20 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:55:20 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:55:21 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:55:21 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 55, 21, 17458),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 55, 20, 777323)}
2017-08-17 16:55:21 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:55:52 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:55:52 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:55:52 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:55:52 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:55:52 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:55:53 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:55:53 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:55:53 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:55:53 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:55:53 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:55:53 [scrapy] INFO: Spider opened
2017-08-17 16:55:53 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:55:53 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:55:53 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:55:53 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:55:53 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:55:53 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:55:53 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:55:53 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:55:53 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 55, 53, 705420),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 55, 53, 471172)}
2017-08-17 16:55:53 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:56:10 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:56:10 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:56:10 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:56:10 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:56:11 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:56:12 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:56:12 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:56:12 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:56:12 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:56:12 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:56:12 [scrapy] INFO: Spider opened
2017-08-17 16:56:12 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:56:12 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:56:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:56:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:56:12 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:56:12 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:56:12 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:56:12 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:56:12 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 56, 12, 273797),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 56, 12, 35422)}
2017-08-17 16:56:12 [scrapy] INFO: Spider closed (finished)
2017-08-17 16:56:58 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 16:56:58 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 16:56:58 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 16:56:58 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 16:56:58 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 16:56:59 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 16:56:59 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 16:56:59 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 16:56:59 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 16:56:59 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 16:56:59 [scrapy] INFO: Spider opened
2017-08-17 16:56:59 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 16:56:59 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 16:56:59 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:56:59 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:56:59 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:56:59 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 16:56:59 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 16:57:00 [scrapy] INFO: Closing spider (finished)
2017-08-17 16:57:00 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 8, 57, 0, 211826),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 8, 56, 59, 949354)}
2017-08-17 16:57:00 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:02:34 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:02:34 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:02:34 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:02:34 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:02:34 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:02:35 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:02:35 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:02:35 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:02:35 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:02:35 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:02:35 [scrapy] INFO: Spider opened
2017-08-17 17:02:35 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:02:35 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:02:35 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:02:35 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:02:35 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:02:35 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:02:35 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:02:38 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:02:39 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:02:39 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0431626574435, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0422684835128, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464859909718, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0438401049329, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0429090982937, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0431638758172, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464141105046, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0417854173655, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0423199187195, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0402231145498, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0405951726343, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.043414281845, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0738006682745
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0753415278566
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0431626574435, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0422684835128, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464859909718, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0438401049329, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0429090982937, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0431638758172, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464141105046, below negative shortcut threshhold 0.05
2017-08-17 17:02:51 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0417854173655, below negative shortcut threshhold 0.05
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0423199187195, below negative shortcut threshhold 0.05
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0402231145498, below negative shortcut threshhold 0.05
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0405951726343, below negative shortcut threshhold 0.05
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.043414281845, below negative shortcut threshhold 0.05
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0738006682745
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0753415278566
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:02:52 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:02:52 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0366998518724, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0366336058409, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0361063022922, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0360742779328, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0477417564316, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0455963889027, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0419593599342
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0379356678884
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0410418380986
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0393872858727
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0700888855946
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0718127029822
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.039904427654
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0427295198773
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0366998518724, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0366336058409, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0361063022922, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0360742779328, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0477417564316, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0455963889027, below negative shortcut threshhold 0.05
2017-08-17 17:03:01 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0419593599342
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0379356678884
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0410418380986
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0393872858727
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0700888855946
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0718127029822
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.039904427654
2017-08-17 17:03:02 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0427295198773
2017-08-17 17:03:02 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:06:30 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 31, in parse
    self.get_web_page(index, item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 39, in get_web_page
    response = urllib2.urlopen(url)
  File "/usr/lib/python2.7/urllib2.py", line 154, in urlopen
    return opener.open(url, data, timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error [Errno -3] Temporary failure in name resolution>
2017-08-17 17:06:30 [scrapy] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:06:30 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:06:30 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 6, 30, 916323),
 'log_count/DEBUG': 174,
 'log_count/ERROR': 4,
 'log_count/INFO': 8,
 'log_count/WARNING': 3,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/URLError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 2, 35, 727655)}
2017-08-17 17:06:30 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:09:11 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:09:11 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:09:11 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:09:11 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:09:11 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:09:12 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:09:12 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:09:12 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:09:12 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:09:12 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:09:12 [scrapy] INFO: Spider opened
2017-08-17 17:09:12 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:09:12 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:09:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:09:12 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:09:12 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:09:12 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:09:12 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:09:27 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:09:28 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:09:28 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0431626574435, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0422684835128, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464859909718, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0438401049329, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0429090982937, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0431638758172, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464141105046, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0417854173655, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0423199187195, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0402231145498, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0405951726343, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.043414281845, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0738006682745
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0753415278566
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0431626574435, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0422684835128, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464859909718, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0438401049329, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0429090982937, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0431638758172, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464141105046, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0417854173655, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0423199187195, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0402231145498, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0405951726343, below negative shortcut threshhold 0.05
2017-08-17 17:09:34 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.043414281845, below negative shortcut threshhold 0.05
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0738006682745
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0753415278566
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:09:35 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:09:35 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:09:43 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 31, in parse
    self.get_web_page(index, item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 39, in get_web_page
    response = urllib2.urlopen(url)
  File "/usr/lib/python2.7/urllib2.py", line 154, in urlopen
    return opener.open(url, data, timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1201, in do_open
    r = h.getresponse(buffering=True)
  File "/usr/lib/python2.7/httplib.py", line 1136, in getresponse
    response.begin()
  File "/usr/lib/python2.7/httplib.py", line 453, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python2.7/httplib.py", line 409, in _read_status
    line = self.fp.readline(_MAXLINE + 1)
  File "/usr/lib/python2.7/socket.py", line 480, in readline
    data = self._sock.recv(self._rbufsize)
error: [Errno 104] Connection reset by peer
2017-08-17 17:09:43 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:09:43 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 9, 43, 465529),
 'log_count/DEBUG': 118,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/error': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 9, 12, 860835)}
2017-08-17 17:09:43 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:12:17 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:12:17 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:12:17 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:12:17 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:12:17 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:12:18 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:12:18 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:12:18 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:12:18 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:12:18 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:12:18 [scrapy] INFO: Spider opened
2017-08-17 17:12:18 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:12:18 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:12:18 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:12:18 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:12:18 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:12:18 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:12:18 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:12:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:12:33 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0431626574435, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0422684835128, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464859909718, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0438401049329, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0429090982937, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0431638758172, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464141105046, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0417854173655, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0423199187195, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0402231145498, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0405951726343, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.043414281845, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0738006682745
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0753415278566
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0431626574435, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0422684835128, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464859909718, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0438401049329, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0429090982937, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0431638758172, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464141105046, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0417854173655, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0423199187195, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0402231145498, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0405951726343, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.043414281845, below negative shortcut threshhold 0.05
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0738006682745
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0753415278566
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:12:58 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:12:58 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:22:21 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:22:21 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:22:21 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:22:21 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:22:21 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:22:22 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:22:22 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:22:22 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:22:22 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:22:22 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:22:22 [scrapy] INFO: Spider opened
2017-08-17 17:22:22 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:22:22 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:22:22 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:22:22 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:22:22 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:22:22 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:22:22 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:22:23 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:22:23 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0431626574435, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0422684835128, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464859909718, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0438401049329, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0429090982937, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0431638758172, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464141105046, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0417854173655, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0423199187195, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0402231145498, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0405951726343, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.043414281845, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0738006682745
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0753415278566
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0431626574435, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0422684835128, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464859909718, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0438401049329, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0429090982937, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0431638758172, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0464141105046, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0417854173655, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0423199187195, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0402231145498, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0405951726343, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.043414281845, below negative shortcut threshhold 0.05
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0738006682745
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0753415278566
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:22:24 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:22:24 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0366998518724, below negative shortcut threshhold 0.05
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0366336058409, below negative shortcut threshhold 0.05
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0361063022922, below negative shortcut threshhold 0.05
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0360742779328, below negative shortcut threshhold 0.05
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0477417564316, below negative shortcut threshhold 0.05
2017-08-17 17:22:25 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0455963889027, below negative shortcut threshhold 0.05
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0419593599342
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0379356678884
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0410418380986
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0393872858727
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0700888855946
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0718127029822
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.039904427654
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0427295198773
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0366998518724, below negative shortcut threshhold 0.05
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0366336058409, below negative shortcut threshhold 0.05
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0361063022922, below negative shortcut threshhold 0.05
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0360742779328, below negative shortcut threshhold 0.05
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0477417564316, below negative shortcut threshhold 0.05
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0455963889027, below negative shortcut threshhold 0.05
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0419593599342
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0379356678884
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0410418380986
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0393872858727
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0700888855946
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0718127029822
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.039904427654
2017-08-17 17:22:26 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0427295198773
2017-08-17 17:22:26 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0285556891336, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0368933320476, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0385229866167, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0326072835372, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0391569383417, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0367448734959, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0401996847831, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0334001120731, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0410575417837, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0435980373876, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0402619197095, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0426503386753, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0600349529775
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0614774631757
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0285556891336, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0368933320476, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0385229866167, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0326072835372, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0391569383417, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0367448734959, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0401996847831, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0334001120731, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: TIS-620 confidence = 0.0410575417837, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0435980373876, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0402619197095, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0426503386753, below negative shortcut threshhold 0.05
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0600349529775
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0614774631757
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: TIS-620 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:24:40 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:24:40 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.0453145586719
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0407321177937
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0367873241054
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0416471696519
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0382185832703
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0409224470951
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0726900040061
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0740368317369
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0329762783222
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0406467099636
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0412564265568
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0411038001938
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0405126723968
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0429193658065
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.0453145586719
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0407321177937
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0367873241054
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0416471696519
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0382185832703
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0409224470951
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0726900040061
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0740368317369
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0329762783222
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0406467099636
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0412564265568
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0411038001938
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0405126723968
2017-08-17 17:24:43 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0429193658065
2017-08-17 17:24:43 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.042561931579, below negative shortcut threshhold 0.05
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0400088566473, below negative shortcut threshhold 0.05
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.043758057982
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0341092283724
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0417091950633
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0385952610492
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0442872010073
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0764346057236
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0766522666281
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0398315095295
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0399601703813
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.036163183585
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0429835131814
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0444283371539
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:47 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.042561931579, below negative shortcut threshhold 0.05
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: ISO-8859-9 confidence = 0.0400088566473, below negative shortcut threshhold 0.05
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.043758057982
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0341092283724
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0417091950633
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0385952610492
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0442872010073
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0764346057236
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0766522666281
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0398315095295
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0399601703813
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.036163183585
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: ISO-8859-9 not active
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0429835131814
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0444283371539
2017-08-17 17:24:48 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0357506516597, below negative shortcut threshhold 0.05
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0443002731027, below negative shortcut threshhold 0.05
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0432178520912, below negative shortcut threshhold 0.05
2017-08-17 17:24:48 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0385032147457, below negative shortcut threshhold 0.05
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.047267663747
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0371892887527
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0470280215672
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0758742824827
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0774844012686
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0417809539194
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0430279501973
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0389707812596
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0395180107205
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0398943727274
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0357506516597, below negative shortcut threshhold 0.05
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0443002731027, below negative shortcut threshhold 0.05
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: IBM866 confidence = 0.0432178520912, below negative shortcut threshhold 0.05
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0385032147457, below negative shortcut threshhold 0.05
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.047267663747
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0371892887527
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: IBM866 not active
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0470280215672
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0758742824827
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0774844012686
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0417809539194
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0430279501973
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0389707812596
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0395180107205
2017-08-17 17:24:49 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0398943727274
2017-08-17 17:24:49 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0410723341712, below negative shortcut threshhold 0.05
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0367415540455, below negative shortcut threshhold 0.05
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0381814758474, below negative shortcut threshhold 0.05
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.041176145985, below negative shortcut threshhold 0.05
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0435697514924, below negative shortcut threshhold 0.05
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0424070864456
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0425022772602
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0445621838589
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.079738468419
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0817419442986
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.045882687912
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0448935137036
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0451621675107
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0432645974472
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0410723341712, below negative shortcut threshhold 0.05
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0367415540455, below negative shortcut threshhold 0.05
2017-08-17 17:24:51 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0381814758474, below negative shortcut threshhold 0.05
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.041176145985, below negative shortcut threshhold 0.05
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0435697514924, below negative shortcut threshhold 0.05
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0424070864456
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0425022772602
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0445621838589
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.079738468419
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0817419442986
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.045882687912
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0448935137036
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0451621675107
2017-08-17 17:24:52 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0432645974472
2017-08-17 17:24:52 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0421517402318, below negative shortcut threshhold 0.05
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0393911096997, below negative shortcut threshhold 0.05
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0405416343589, below negative shortcut threshhold 0.05
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.039676383285, below negative shortcut threshhold 0.05
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0396820627201
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0398659694301
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0745307301107
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0764140659666
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0428537814384
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426468239985
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0378655473158
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0444827251306
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0478916471419
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0478916471419
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0421517402318, below negative shortcut threshhold 0.05
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0393911096997, below negative shortcut threshhold 0.05
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: MacCyrillic confidence = 0.0405416343589, below negative shortcut threshhold 0.05
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.039676383285, below negative shortcut threshhold 0.05
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0396820627201
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: MacCyrillic not active
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0398659694301
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0745307301107
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0764140659666
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0428537814384
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426468239985
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0378655473158
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0444827251306
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0478916471419
2017-08-17 17:24:57 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0478916471419
2017-08-17 17:24:57 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.0367764327071
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0308106453409
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0409862940062
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0377003290581
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0334856423968
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0417909677542
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0751233246377
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0764689604386
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0405356922339
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.039194751786
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427769603407
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0401156134338
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0550489030399
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0569603232844
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:32 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.0367764327071
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0308106453409
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0409862940062
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0377003290581
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0334856423968
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0417909677542
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0751233246377
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0764689604386
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0405356922339
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.039194751786
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427769603407
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0401156134338
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0550489030399
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0569603232844
2017-08-17 17:25:33 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:33 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0401593629104, below negative shortcut threshhold 0.05
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.0481052578889
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0411158370785
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0333113684611
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0490832698958
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0402062499323
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0653045165905
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0674612406419
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0330817944064
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0490467753321
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0443744716909
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0370950502736
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0510710762061
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.048801250597
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0401593629104, below negative shortcut threshhold 0.05
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.0481052578889
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0411158370785
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0333113684611
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0490832698958
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0402062499323
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0653045165905
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0674612406419
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0330817944064
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0490467753321
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0443744716909
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0370950502736
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0510710762061
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.048801250597
2017-08-17 17:25:34 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0388730805351, below negative shortcut threshhold 0.05
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0435553703321, below negative shortcut threshhold 0.05
2017-08-17 17:25:34 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0386137054367, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0448766579798, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0406398009752, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0424645090749, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0446237214008, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0428069105112
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0409089728294
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0812117866145
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0819250911109
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0379657454056
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0459739618511
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0408763801178
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0388730805351, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0435553703321, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0386137054367, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: IBM855 confidence = 0.0448766579798, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0406398009752, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0424645090749, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 confidence = 0.0446237214008, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0428069105112
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0409089728294
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: IBM855 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0812117866145
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0819250911109
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0379657454056
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0459739618511
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0408763801178
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 not active
2017-08-17 17:25:35 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0499744753563, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0376489773078, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0387959909548, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0520186463582
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0391983209133
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0399564199848
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0754280774658
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0771316824985
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0420296928536
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0468973357265
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0472163707224
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0416544495267
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0392043306748
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0399303367984
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: windows-1251 confidence = 0.0499744753563, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: KOI8-R confidence = 0.0376489773078, below negative shortcut threshhold 0.05
2017-08-17 17:25:35 [chardet.charsetprober] DEBUG: ISO-8859-5 confidence = 0.0387959909548, below negative shortcut threshhold 0.05
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1251 not active
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: KOI8-R not active
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: ISO-8859-5 not active
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0520186463582
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0391983209133
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0399564199848
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0754280774658
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0771316824985
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0420296928536
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0468973357265
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0472163707224
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0416544495267
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0392043306748
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0399303367984
2017-08-17 17:25:36 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.0448480582203
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0363861270266
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0425735513321
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.045312508677
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0402017278552
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0428914868633
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.077208003295
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0775896647112
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0385893320063
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0384358501208
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0375285709068
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0451651563399
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0448013554629
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.042783276388
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:25:36 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.0448480582203
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0363861270266
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0425735513321
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.045312508677
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0402017278552
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0428914868633
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.077208003295
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0775896647112
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0385893320063
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0384358501208
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0375285709068
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0451651563399
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0448013554629
2017-08-17 17:25:37 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.042783276388
2017-08-17 17:25:37 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:25:37 [scrapy] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:25:37 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:25:37 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 25, 37, 471269),
 'log_count/DEBUG': 734,
 'log_count/ERROR': 3,
 'log_count/INFO': 8,
 'log_count/WARNING': 14,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 22, 22, 699978)}
2017-08-17 17:25:37 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:28:22 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:28:22 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:28:22 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:28:22 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:28:22 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:28:23 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:28:23 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:28:23 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:28:23 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:28:23 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:28:23 [scrapy] INFO: Spider opened
2017-08-17 17:28:23 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:28:23 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:28:23 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:28:23 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:28:23 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:28:23 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:28:23 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:28:27 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:28:27 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 28, 27, 694509),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 28, 23, 654361)}
2017-08-17 17:28:27 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:28:35 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:28:35 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:28:35 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:28:35 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:28:35 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:28:36 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:28:36 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:28:36 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:28:36 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:28:36 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:28:36 [scrapy] INFO: Spider opened
2017-08-17 17:28:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:28:36 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:28:36 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:28:36 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:28:36 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:28:36 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:28:36 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:28:36 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:28:36 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 28, 36, 872572),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 28, 36, 679342)}
2017-08-17 17:28:36 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:29:56 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:29:56 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:29:56 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:29:56 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:29:56 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:29:57 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:29:57 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:29:57 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:29:57 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:29:57 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:29:57 [scrapy] INFO: Spider opened
2017-08-17 17:29:57 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:29:57 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:29:57 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:29:57 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:29:57 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:29:57 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:29:57 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:29:58 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:29:58 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:29:58 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:29:58 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 29, 58, 805171),
 'log_count/DEBUG': 50,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 29, 57, 868612)}
2017-08-17 17:29:58 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:31:39 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:31:39 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:31:39 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:31:39 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:31:39 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:31:40 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:31:40 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:31:40 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:31:40 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:31:40 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:31:40 [scrapy] INFO: Spider opened
2017-08-17 17:31:40 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:31:40 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:31:40 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:31:40 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:31:40 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:31:40 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:31:40 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:31:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:31:46 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:31:46 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:31:46 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 31, 46, 851740),
 'log_count/DEBUG': 50,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 31, 40, 968732)}
2017-08-17 17:31:46 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:34:27 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:34:27 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:34:27 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:34:27 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:34:27 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:34:28 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:34:28 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:34:28 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:34:28 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:34:28 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:34:28 [scrapy] INFO: Spider opened
2017-08-17 17:34:28 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:34:28 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:34:28 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:34:28 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:34:28 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:34:28 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:34:28 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:34:29 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:34:29 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:34:29 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:34:29 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 34, 29, 635905),
 'log_count/DEBUG': 50,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 34, 28, 201361)}
2017-08-17 17:34:29 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:35:20 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:35:20 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:35:20 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:35:21 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:35:21 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:35:22 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:35:22 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:35:22 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:35:22 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:35:22 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:35:22 [scrapy] INFO: Spider opened
2017-08-17 17:35:22 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:35:22 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:35:22 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:35:22 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:35:22 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:35:22 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:35:22 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:35:27 [scrapy] ERROR: Spider error processing <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 31, in parse
    self.get_web_page(index, item)
  File "/home/lwq/Desktop/lwq/my/my/spiders/runoob.py", line 41, in get_web_page
    html = response.read().decode('utf-8')
  File "/usr/lib/python2.7/encodings/utf_8.py", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x8b in position 1: invalid start byte
2017-08-17 17:35:27 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:35:27 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 35, 27, 657608),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeDecodeError': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 35, 22, 151018)}
2017-08-17 17:35:27 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:37:30 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:37:30 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:37:30 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:37:30 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:37:30 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:37:31 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:37:31 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:37:31 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:37:31 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:37:31 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:37:31 [scrapy] INFO: Spider opened
2017-08-17 17:37:31 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:37:31 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:37:31 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:37:31 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:37:31 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:37:31 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:37:31 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese prober hit error at byte 2
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: EUC-JP Japanese prober hit error at byte 1
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: GB2312 Chinese prober hit error at byte 2
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: EUC-KR Korean prober hit error at byte 1
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: CP949 Korean prober hit error at byte 2
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: Big5 Chinese prober hit error at byte 1
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: EUC-TW Taiwan prober hit error at byte 1
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.040114544151
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.0345972704899
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0408815701922
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.0439581948991
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.0339830218232
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.0353744312253
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0762444156391
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0779249985307
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.040277900226
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0426140295295
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.0427473616599
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.0488800826162
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0413533835874
2017-08-17 17:37:36 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0417293234382
2017-08-17 17:37:36 [bs4.dammit] WARNING: Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
2017-08-17 17:37:36 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:37:36 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 37, 36, 568852),
 'log_count/DEBUG': 50,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 37, 31, 420898)}
2017-08-17 17:37:36 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:38:30 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:38:30 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:38:30 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:38:30 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:38:30 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:38:31 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:38:31 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:38:31 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:38:31 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:38:31 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:38:31 [scrapy] INFO: Spider opened
2017-08-17 17:38:31 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:38:31 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:38:31 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:38:31 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:38:31 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:38:31 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:38:31 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:38:32 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:38:32 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 38, 32, 166778),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 38, 31, 955558)}
2017-08-17 17:38:32 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:39:55 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:39:55 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:39:55 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:39:55 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:39:55 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:39:56 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:39:56 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:39:56 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:39:56 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:39:56 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:39:56 [scrapy] INFO: Spider opened
2017-08-17 17:39:56 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:39:56 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:39:56 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:39:56 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:39:56 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:39:56 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:39:56 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:39:56 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:39:56 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 39, 56, 767885),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 39, 56, 580153)}
2017-08-17 17:39:56 [scrapy] INFO: Spider closed (finished)
2017-08-17 17:49:51 [scrapy] INFO: Scrapy 1.0.3 started (bot: my)
2017-08-17 17:49:51 [scrapy] INFO: Optional features available: ssl, http11, boto
2017-08-17 17:49:51 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'my.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['my.spiders'], 'LOG_FILE': 'my.log', 'BOT_NAME': 'my'}
2017-08-17 17:49:51 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2017-08-17 17:49:52 [boto] DEBUG: Retrieving credentials from metadata server.
2017-08-17 17:49:53 [boto] ERROR: Caught exception reading instance data
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/boto/utils.py", line 210, in retry_url
    r = opener.open(req, timeout=timeout)
  File "/usr/lib/python2.7/urllib2.py", line 429, in open
    response = self._open(req, data)
  File "/usr/lib/python2.7/urllib2.py", line 447, in _open
    '_open', req)
  File "/usr/lib/python2.7/urllib2.py", line 407, in _call_chain
    result = func(*args)
  File "/usr/lib/python2.7/urllib2.py", line 1228, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File "/usr/lib/python2.7/urllib2.py", line 1198, in do_open
    raise URLError(err)
URLError: <urlopen error timed out>
2017-08-17 17:49:53 [boto] ERROR: Unable to read instance data, giving up
2017-08-17 17:49:53 [scrapy] INFO: Enabled downloader middlewares: RobotsTxtMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2017-08-17 17:49:53 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2017-08-17 17:49:53 [scrapy] INFO: Enabled item pipelines: 
2017-08-17 17:49:53 [scrapy] INFO: Spider opened
2017-08-17 17:49:53 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-08-17 17:49:53 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-08-17 17:49:53 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 1 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:49:53 [scrapy] DEBUG: Retrying <GET file:///robots.txt> (failed 2 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:49:53 [scrapy] DEBUG: Gave up retrying <GET file:///robots.txt> (failed 3 times): [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:49:53 [scrapy] ERROR: Error downloading <GET file:///robots.txt>: [Errno 2] No such file or directory: '/robots.txt'
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/file.py", line 13, in download_request
    body = open(filepath, 'rb').read()
IOError: [Errno 2] No such file or directory: '/robots.txt'
2017-08-17 17:49:53 [scrapy] DEBUG: Crawled (200) <GET file:///home/lwq/Desktop/lwq/my/html/homepage.html> (referer: None)
2017-08-17 17:50:15 [scrapy] INFO: Closing spider (finished)
2017-08-17 17:50:15 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/exceptions.IOError': 3,
 'downloader/request_bytes': 880,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 59874,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 8, 17, 9, 50, 15, 516682),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 8, 17, 9, 49, 53, 51779)}
2017-08-17 17:50:15 [scrapy] INFO: Spider closed (finished)
